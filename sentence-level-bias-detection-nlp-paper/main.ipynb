{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rlq14aBmDYCsTbiKZnb7c7Ww_SqPYvfo","authorship_tag":"ABX9TyPuveIUu6OIHFAg1qpFdzgH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium","widgets":{"application/vnd.jupyter.widget-state+json":{"0a9b8d3558bc46eda27648d8eed6610d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_772a17563d5a4a278b63f0fb5956f71a","IPY_MODEL_8f56816994794a71b25bc76ee1a788e7","IPY_MODEL_3ee3d040cf98400683b9c3f99a891179"],"layout":"IPY_MODEL_095b3e6e68414dc2a670c53344086262"}},"772a17563d5a4a278b63f0fb5956f71a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43f8af40d16a496db390a4fec8b23878","placeholder":"​","style":"IPY_MODEL_4d6db122762f476187e2002f15b8e4ec","value":"Downloading: 100%"}},"8f56816994794a71b25bc76ee1a788e7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_03981226032a448d823a91b3926c32d5","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3bc636dc2fab4e8db80c96327cf789aa","value":898823}},"3ee3d040cf98400683b9c3f99a891179":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_859edd4cf2aa462ab5cbe50c069a1181","placeholder":"​","style":"IPY_MODEL_ea3d1024fb2546ed9f203a86a9492b68","value":" 899k/899k [00:00&lt;00:00, 1.68MB/s]"}},"095b3e6e68414dc2a670c53344086262":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43f8af40d16a496db390a4fec8b23878":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d6db122762f476187e2002f15b8e4ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03981226032a448d823a91b3926c32d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bc636dc2fab4e8db80c96327cf789aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"859edd4cf2aa462ab5cbe50c069a1181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea3d1024fb2546ed9f203a86a9492b68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"532944fd01bf4466a02c5550902a64df":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_027f51d4c13f481d9ce094b723226ecd","IPY_MODEL_053191a6d58344bf9653d8657f847040","IPY_MODEL_a19db6a1337f4bbc822b696e238a8f11"],"layout":"IPY_MODEL_85b538594133412c8150400656bc50bb"}},"027f51d4c13f481d9ce094b723226ecd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f42f2acdd1ce4274aa8a52a1ee67c008","placeholder":"​","style":"IPY_MODEL_551e1383a062485993f3d63114cf7774","value":"Downloading: 100%"}},"053191a6d58344bf9653d8657f847040":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40346bcad754468f89db8364811c6a95","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7d48c00e17640b28116b1fffc03baab","value":456318}},"a19db6a1337f4bbc822b696e238a8f11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4983b3f0e826452ca11906d6d8c93d01","placeholder":"​","style":"IPY_MODEL_18a8678a989447f4a56f53c42fc9c798","value":" 456k/456k [00:00&lt;00:00, 1.68MB/s]"}},"85b538594133412c8150400656bc50bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f42f2acdd1ce4274aa8a52a1ee67c008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"551e1383a062485993f3d63114cf7774":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40346bcad754468f89db8364811c6a95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7d48c00e17640b28116b1fffc03baab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4983b3f0e826452ca11906d6d8c93d01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18a8678a989447f4a56f53c42fc9c798":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae30554a34f249518c6d4c1bb7609d10":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14fea4162fb74ba69df492974b3dd36d","IPY_MODEL_1de0bba3e23a45a5b804053308d5e473","IPY_MODEL_9cf3b3858cf54252b4815cfeb733c628"],"layout":"IPY_MODEL_b1eef2786e134653862d7804350d722a"}},"14fea4162fb74ba69df492974b3dd36d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49a77c7a86cb4cdab27fa08f17c9307d","placeholder":"​","style":"IPY_MODEL_429794215e524dd1a9d1c43f5c93a2f8","value":"Downloading: 100%"}},"1de0bba3e23a45a5b804053308d5e473":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aee12247cc994ab9a7a408c5f6a7ca00","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7059863d79a7488db133ef0a2c21988d","value":481}},"9cf3b3858cf54252b4815cfeb733c628":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6c674031e3641cdbb5eb610e8b26a73","placeholder":"​","style":"IPY_MODEL_e5a343e98b5c427fb03480a197829284","value":" 481/481 [00:00&lt;00:00, 18.2kB/s]"}},"b1eef2786e134653862d7804350d722a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49a77c7a86cb4cdab27fa08f17c9307d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"429794215e524dd1a9d1c43f5c93a2f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aee12247cc994ab9a7a408c5f6a7ca00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7059863d79a7488db133ef0a2c21988d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b6c674031e3641cdbb5eb610e8b26a73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5a343e98b5c427fb03480a197829284":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5082245995d4a2493443cfefd5041ae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dee963d1224b4d19ad76fa254a9f99bc","IPY_MODEL_960473289b7e4323b85f94bd4b42ed48","IPY_MODEL_98810ab376874eedba4c0d7dae636bee"],"layout":"IPY_MODEL_2b0878d76b2e471fbe96d6d9dab03a13"}},"dee963d1224b4d19ad76fa254a9f99bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45da057a65954e5e8d89e9c2c009e8f5","placeholder":"​","style":"IPY_MODEL_e86671379b404531853ff9fc128715b7","value":"Downloading: 100%"}},"960473289b7e4323b85f94bd4b42ed48":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab53f6411abc48d199bf28e83c43afc3","max":657434796,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b20dafc73ff41e4a0c7175968ed078c","value":657434796}},"98810ab376874eedba4c0d7dae636bee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_846358357c63405692183ccbeb6ae217","placeholder":"​","style":"IPY_MODEL_6da66d0b5dee47d9baa0f3ab19a30353","value":" 657M/657M [00:13&lt;00:00, 49.4MB/s]"}},"2b0878d76b2e471fbe96d6d9dab03a13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45da057a65954e5e8d89e9c2c009e8f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e86671379b404531853ff9fc128715b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab53f6411abc48d199bf28e83c43afc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b20dafc73ff41e4a0c7175968ed078c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"846358357c63405692183ccbeb6ae217":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6da66d0b5dee47d9baa0f3ab19a30353":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62d068efc27d4206ba2613cd2e57868b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47d8c7d851ea429d89c0642a0c0e7b51","IPY_MODEL_094552b53b524060a5abc321c2106ed5","IPY_MODEL_469b9a6a06df49788dc4a6214022e9bc"],"layout":"IPY_MODEL_872c292836024bbb93750aa98e6572fb"}},"47d8c7d851ea429d89c0642a0c0e7b51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d264455d09b4a638418e5a11a7d43fc","placeholder":"​","style":"IPY_MODEL_e9410d8174a34e7c9448f5c1292d88d3","value":"Downloading: 100%"}},"094552b53b524060a5abc321c2106ed5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcd5fd2cb1124af0bfb382a6de78d732","max":898825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6252306613ae4480b2fa4004f3d1a1b9","value":898825}},"469b9a6a06df49788dc4a6214022e9bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bef339c773d84e8885fc2be8b443ea66","placeholder":"​","style":"IPY_MODEL_85e87e67c3b64c29891a8be64bf28935","value":" 899k/899k [00:00&lt;00:00, 1.74MB/s]"}},"872c292836024bbb93750aa98e6572fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d264455d09b4a638418e5a11a7d43fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9410d8174a34e7c9448f5c1292d88d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcd5fd2cb1124af0bfb382a6de78d732":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6252306613ae4480b2fa4004f3d1a1b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bef339c773d84e8885fc2be8b443ea66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85e87e67c3b64c29891a8be64bf28935":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a04b5dd4737b4918891429fe8a13a3ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d4a1915ca5f742f1aaa6b0d11c6edd8a","IPY_MODEL_87257dcb104e4ddeba679370d3ed4e9b","IPY_MODEL_e35073a346fd40e795ad93d6de8ca1b1"],"layout":"IPY_MODEL_74fb3860c92c44689e64acef5662190a"}},"d4a1915ca5f742f1aaa6b0d11c6edd8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4fafe3fbf8e4936960c82ea9611ca04","placeholder":"​","style":"IPY_MODEL_ed4ae93eee584e29bfd82363e62a308a","value":"Downloading: 100%"}},"87257dcb104e4ddeba679370d3ed4e9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f6bdbf84def4c85bb1753c4cd9ac7b2","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76946672010a4d6eac6dd8a92aba4446","value":456318}},"e35073a346fd40e795ad93d6de8ca1b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_024fea91f71d44c2bf491d4f7c88ccca","placeholder":"​","style":"IPY_MODEL_424bdad8c00448c8a68fcf2fff1caa00","value":" 456k/456k [00:00&lt;00:00, 1.60MB/s]"}},"74fb3860c92c44689e64acef5662190a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4fafe3fbf8e4936960c82ea9611ca04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed4ae93eee584e29bfd82363e62a308a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f6bdbf84def4c85bb1753c4cd9ac7b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76946672010a4d6eac6dd8a92aba4446":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"024fea91f71d44c2bf491d4f7c88ccca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"424bdad8c00448c8a68fcf2fff1caa00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"101b545463824ad899ce947536da6e42":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b0a638955aa4757aa0398a3b7ad2b19","IPY_MODEL_55b9ac7cabb84da4a95a1034cab73256","IPY_MODEL_35c4b54bc9324eaabe49590ef3997ca7"],"layout":"IPY_MODEL_9726116f7bf14ab886e0f2e37b7a7c60"}},"4b0a638955aa4757aa0398a3b7ad2b19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb1a537ada154ddd973b81a876e5ef66","placeholder":"​","style":"IPY_MODEL_37133930c2c545fb826287f03344d9d3","value":"Downloading: 100%"}},"55b9ac7cabb84da4a95a1034cab73256":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e027605e1e94fc385984bac22727698","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aa144279e8d3423199ebaaae154572a1","value":52}},"35c4b54bc9324eaabe49590ef3997ca7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_adf29755acd440e79048feb8ca59f4c6","placeholder":"​","style":"IPY_MODEL_b2704c485fcc4d70b04ea9d7ea682775","value":" 52.0/52.0 [00:00&lt;00:00, 1.83kB/s]"}},"9726116f7bf14ab886e0f2e37b7a7c60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb1a537ada154ddd973b81a876e5ef66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37133930c2c545fb826287f03344d9d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e027605e1e94fc385984bac22727698":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa144279e8d3423199ebaaae154572a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"adf29755acd440e79048feb8ca59f4c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2704c485fcc4d70b04ea9d7ea682775":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f24c9ac19c28460eb81e956e1c7f3c0a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3cbd8bef48be4542a79a6fba273eb85e","IPY_MODEL_4374521eb0f34620ab49a812aa23ef3e","IPY_MODEL_dddf2d34cbae4958a06b122dfafea30f"],"layout":"IPY_MODEL_eb66b0febd3e4bf08e9c9fe6f7fecc73"}},"3cbd8bef48be4542a79a6fba273eb85e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf84f370b462424abe460efd7fd24f83","placeholder":"​","style":"IPY_MODEL_b3005df58ee74187a0ad92b5545f6b57","value":"Downloading: 100%"}},"4374521eb0f34620ab49a812aa23ef3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f2dc197e90f42509cbf38e3a13b088b","max":744,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf1e566b0eda4b37b5a373f7604e66b5","value":744}},"dddf2d34cbae4958a06b122dfafea30f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3331c82556bf4f68b5c66a48b89c3bd7","placeholder":"​","style":"IPY_MODEL_8341cb6947f14dcdac638d0eb0f7c078","value":" 744/744 [00:00&lt;00:00, 26.6kB/s]"}},"eb66b0febd3e4bf08e9c9fe6f7fecc73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf84f370b462424abe460efd7fd24f83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3005df58ee74187a0ad92b5545f6b57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f2dc197e90f42509cbf38e3a13b088b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf1e566b0eda4b37b5a373f7604e66b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3331c82556bf4f68b5c66a48b89c3bd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8341cb6947f14dcdac638d0eb0f7c078":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"121cde92b9034e07be1a8c02706f4dd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0467a7154a34c4e936c5e8e62396d38","IPY_MODEL_cb69b906fb684c40815d7e35b552c546","IPY_MODEL_48943b6ceda6461490467cfddf994f37"],"layout":"IPY_MODEL_e2226c4e87da4d9bb26565d07412fa1d"}},"e0467a7154a34c4e936c5e8e62396d38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2308803984b486fa93be5907f4fe6f2","placeholder":"​","style":"IPY_MODEL_97f53a47b69e41639ab5f59fcf2361be","value":"Downloading: 100%"}},"cb69b906fb684c40815d7e35b552c546":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe2c7f531f624c548ed2b1ee864e71df","max":554665712,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80cbee57f3d34e0f9320fb72f0b406d6","value":554665712}},"48943b6ceda6461490467cfddf994f37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe74522a83f24df199136add8daa966b","placeholder":"​","style":"IPY_MODEL_42e617f3e6b3416fa9aa625bd74ab1ef","value":" 555M/555M [00:11&lt;00:00, 39.2MB/s]"}},"e2226c4e87da4d9bb26565d07412fa1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2308803984b486fa93be5907f4fe6f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97f53a47b69e41639ab5f59fcf2361be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe2c7f531f624c548ed2b1ee864e71df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80cbee57f3d34e0f9320fb72f0b406d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe74522a83f24df199136add8daa966b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42e617f3e6b3416fa9aa625bd74ab1ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"415bf4e128cb48b7a026f6d5b6cc2dad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd17fb25426045b5b1dd49df73211cff","IPY_MODEL_080e90a57fcf43c59e48d3cf670835be","IPY_MODEL_7926399739ed45f18b239d169fd1903f"],"layout":"IPY_MODEL_762cf7566f2d48d78a8e56b165c2bdab"}},"cd17fb25426045b5b1dd49df73211cff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba694602ca3d41469d574c01ff35f7b2","placeholder":"​","style":"IPY_MODEL_594dad76ae234b2fb5638cf3e811b25d","value":"Downloading: 100%"}},"080e90a57fcf43c59e48d3cf670835be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_34129e37b8c349e08706f7b973d46c10","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8d815ec90ee4f9696152ec57044f99b","value":231508}},"7926399739ed45f18b239d169fd1903f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea3792d9933446bfb3a4108dee03dfaf","placeholder":"​","style":"IPY_MODEL_6e13352920d84ba480e7809cbe607642","value":" 232k/232k [00:00&lt;00:00, 1.42MB/s]"}},"762cf7566f2d48d78a8e56b165c2bdab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba694602ca3d41469d574c01ff35f7b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"594dad76ae234b2fb5638cf3e811b25d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34129e37b8c349e08706f7b973d46c10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8d815ec90ee4f9696152ec57044f99b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea3792d9933446bfb3a4108dee03dfaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e13352920d84ba480e7809cbe607642":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c54a21945b8c4b4488c7c4e2a6f2c15c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d66293f93c04c418a6718341a1b3bbf","IPY_MODEL_146d8d6ea27a42c3af466e63415dd869","IPY_MODEL_e557b41c02f54122bf86203b4c275b25"],"layout":"IPY_MODEL_99e9001e227c4b2890b414c5a5a801cc"}},"4d66293f93c04c418a6718341a1b3bbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03348d8d75664f63bbda278155067d02","placeholder":"​","style":"IPY_MODEL_67aa05b103a94b03a9b3e7765855ecae","value":"Downloading: 100%"}},"146d8d6ea27a42c3af466e63415dd869":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a93b1cabfe14837bab746eca0971a8b","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_31f7e13cf6604de584edc616930418e8","value":28}},"e557b41c02f54122bf86203b4c275b25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f4040edd6c34717b3d9088f137297bd","placeholder":"​","style":"IPY_MODEL_2cde9e52c065401788a4d54bfa64cdda","value":" 28.0/28.0 [00:00&lt;00:00, 921B/s]"}},"99e9001e227c4b2890b414c5a5a801cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03348d8d75664f63bbda278155067d02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67aa05b103a94b03a9b3e7765855ecae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a93b1cabfe14837bab746eca0971a8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31f7e13cf6604de584edc616930418e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f4040edd6c34717b3d9088f137297bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cde9e52c065401788a4d54bfa64cdda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c015d41835243f4b38f7e6a4c4cd0cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7465abfb215b4a7ba5de3f0a0cd9f833","IPY_MODEL_ad386fbf477a458db925a67ba13f39b8","IPY_MODEL_425c72a0af0346a8a38e5e67aa8516df"],"layout":"IPY_MODEL_602b66553ae04aa296770d47ad633020"}},"7465abfb215b4a7ba5de3f0a0cd9f833":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_047e6af9e080451f84d415293b2d2149","placeholder":"​","style":"IPY_MODEL_2a85da80e2154a00b91fb7980214c78e","value":"Downloading: 100%"}},"ad386fbf477a458db925a67ba13f39b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5eee8cce7854b26976f1f2ae95133ac","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f34183e9c6394dc59a8d75544157405d","value":231508}},"425c72a0af0346a8a38e5e67aa8516df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be370a7859ce4f0bafeb02f7fbcad03f","placeholder":"​","style":"IPY_MODEL_d2a4dc2860f2404a8303ff569e6dfd81","value":" 232k/232k [00:00&lt;00:00, 1.66MB/s]"}},"602b66553ae04aa296770d47ad633020":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"047e6af9e080451f84d415293b2d2149":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a85da80e2154a00b91fb7980214c78e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5eee8cce7854b26976f1f2ae95133ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f34183e9c6394dc59a8d75544157405d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be370a7859ce4f0bafeb02f7fbcad03f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2a4dc2860f2404a8303ff569e6dfd81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edcb1d91c9b84fd7ba6952572a4e3267":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f5bd1e7e03440efbee4c92f6a991e3d","IPY_MODEL_e7d1cf6b3bb94230aa7205e0306cb49c","IPY_MODEL_88d4099af65b4ee0bc0089597fcf371b"],"layout":"IPY_MODEL_177f17ca8f1d46eebf47fbb25caf0ac0"}},"0f5bd1e7e03440efbee4c92f6a991e3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5e9ab0093784138a8b5d6f090df1afe","placeholder":"​","style":"IPY_MODEL_a5fbd0c2db904bdeb35b281f8ed6f8a6","value":"Downloading: 100%"}},"e7d1cf6b3bb94230aa7205e0306cb49c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44bab96895264916af5fc80cc6887ccf","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c51342dfa13468390ef9fb337faefe4","value":29}},"88d4099af65b4ee0bc0089597fcf371b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2240b95d3e1f4ef8ae036df55d9f6ba5","placeholder":"​","style":"IPY_MODEL_c3a1e6be4ae9452db4ccbe901ce10df5","value":" 29.0/29.0 [00:00&lt;00:00, 1.09kB/s]"}},"177f17ca8f1d46eebf47fbb25caf0ac0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5e9ab0093784138a8b5d6f090df1afe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5fbd0c2db904bdeb35b281f8ed6f8a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44bab96895264916af5fc80cc6887ccf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c51342dfa13468390ef9fb337faefe4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2240b95d3e1f4ef8ae036df55d9f6ba5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3a1e6be4ae9452db4ccbe901ce10df5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a49d1c33803a467d812ac0cb5998e62f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ea8fef3962c4c70952e14970597dbbf","IPY_MODEL_b25d2f4791374ad390ed0baa2b8a85e1","IPY_MODEL_e7486fcd7ca048fba45106578653e369"],"layout":"IPY_MODEL_eaf6ae4b655143b7b6dfec4d6a16f860"}},"6ea8fef3962c4c70952e14970597dbbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fb5035de24b4a13b4091a919fc275c0","placeholder":"​","style":"IPY_MODEL_11d64a38a2f44976a9b9fba42d756fa5","value":"Downloading: 100%"}},"b25d2f4791374ad390ed0baa2b8a85e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2574291c9a2543139e49eecb81fc0a76","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d80da758aae641c6ab35e9ba12343d65","value":665}},"e7486fcd7ca048fba45106578653e369":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_653cd5ee87c342f4bb4fc7205ca16e38","placeholder":"​","style":"IPY_MODEL_96ede6392db545339d4381e7ff3ff363","value":" 665/665 [00:00&lt;00:00, 24.6kB/s]"}},"eaf6ae4b655143b7b6dfec4d6a16f860":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fb5035de24b4a13b4091a919fc275c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11d64a38a2f44976a9b9fba42d756fa5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2574291c9a2543139e49eecb81fc0a76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d80da758aae641c6ab35e9ba12343d65":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"653cd5ee87c342f4bb4fc7205ca16e38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96ede6392db545339d4381e7ff3ff363":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ca2fdaf8b6d4b6d94fca3f40b3f2740":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c88786990154517992f46cc18488c0b","IPY_MODEL_3e7e2d3e3e2e4880af79ef842d044c2c","IPY_MODEL_a7107029869945faaab1c3e5de647270"],"layout":"IPY_MODEL_31472b5fa23b4cf5a5b55fe3df21f374"}},"3c88786990154517992f46cc18488c0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e143763f05a64f66852e6f45d1479107","placeholder":"​","style":"IPY_MODEL_718d12c25f6e46eaa0fb23cae314fd92","value":"Downloading: 100%"}},"3e7e2d3e3e2e4880af79ef842d044c2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b7b9441af1d45bb80a7e010a88e4f47","max":54466044,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db9be2f51c9b426d9b7854a4de56c5d3","value":54466044}},"a7107029869945faaab1c3e5de647270":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83034783aba94ac89dc8bbf9c43dde4d","placeholder":"​","style":"IPY_MODEL_9336b434a21c4c5290dcdb250096b47c","value":" 54.5M/54.5M [00:01&lt;00:00, 37.1MB/s]"}},"31472b5fa23b4cf5a5b55fe3df21f374":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e143763f05a64f66852e6f45d1479107":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"718d12c25f6e46eaa0fb23cae314fd92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b7b9441af1d45bb80a7e010a88e4f47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db9be2f51c9b426d9b7854a4de56c5d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83034783aba94ac89dc8bbf9c43dde4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9336b434a21c4c5290dcdb250096b47c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ddEprhrBbzLt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668972270412,"user_tz":420,"elapsed":16673,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"b59c8387-6c86-4803-c32d-0b58a497f60b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 70.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 70.5 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 4.5 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (3.0.10)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install openpyxl"]},{"cell_type":"code","source":["import sys\n","import os\n","import time\n","import re\n","import random\n","from typing import Dict, List, Optional, Union\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import gc\n","\n","from google.colab import files\n","\n","from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import tensorflow as tf\n","from transformers import BertTokenizer, BertConfig, TFBertForSequenceClassification\n","from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n","from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n","from transformers import ElectraTokenizer, TFElectraForSequenceClassification\n","from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n","from transformers import LongformerTokenizer, TFLongformerForSequenceClassification\n","from transformers import DebertaTokenizer, TFDebertaForSequenceClassification"],"metadata":{"id":"x4SI4-L_oZ8s","executionInfo":{"status":"ok","timestamp":1668972275728,"user_tz":420,"elapsed":5320,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# set seed, TF uses python ramdom and numpy library, so these must also be fixed\n","tf.random.set_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","os.environ['PYTHONHASHSEED']=str(0)\n","os.environ['TF_DETERMINISTIC_OPS'] = '0'"],"metadata":{"id":"RhRpvFGhofj9","executionInfo":{"status":"ok","timestamp":1668972275728,"user_tz":420,"elapsed":3,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["PATH_sg1 = \"/content/drive/MyDrive/Colab Notebooks/data/final_labels_SG1.xlsx\"\n","PATH_sg2 = \"/content/drive/MyDrive/Colab Notebooks/data/final_labels_SG2.xlsx\"\n","df_sg1 = pd.read_excel(PATH_sg1)\n","df_sg2 = pd.read_excel(PATH_sg2)\n","df_sg1.rename(columns={'text': 'sentence', 'label_bias': 'Label_bias'}, inplace=True)\n","df_sg2.rename(columns={'text': 'sentence', 'label_bias': 'Label_bias'}, inplace=True)\n","df_sg1.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"7RpkXqjVogCH","executionInfo":{"status":"ok","timestamp":1668972278439,"user_tz":420,"elapsed":2714,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"1cdd14b3-bad0-4f73-cbed-aca0c9cbb92d"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            sentence  \\\n","0  The Republican president assumed he was helpin...   \n","1  Though the indictment of a woman for her own p...   \n","2  Ingraham began the exchange by noting American...   \n","3  The tragedy of America’s 18 years in Afghanist...   \n","4  The justices threw out a challenge from gun ri...   \n","\n","                                           news_link     outlet  \\\n","0  http://www.msnbc.com/rachel-maddow-show/auto-i...      msnbc   \n","1  https://eu.usatoday.com/story/news/nation/2019...  usa-today   \n","2  https://www.breitbart.com/economy/2020/01/12/d...  breitbart   \n","3  http://feedproxy.google.com/~r/breitbart/~3/ER...  breitbart   \n","4  https://www.huffpost.com/entry/supreme-court-g...      msnbc   \n","\n","                                   topic    type    Label_bias  \\\n","0                            environment    left        Biased   \n","1                               abortion  center    Non-biased   \n","2                            immigration   right  No agreement   \n","3  international-politics-and-world-news   right        Biased   \n","4                            gun-control    left    Non-biased   \n","\n","                           label_opinion             biased_words  \n","0             Expresses writer’s opinion                       []  \n","1  Somewhat factual but also opinionated                       []  \n","2                           No agreement                ['flood']  \n","3  Somewhat factual but also opinionated  ['tragedy', 'stubborn']  \n","4                       Entirely factual                       []  "],"text/html":["\n","  <div id=\"df-8966680a-3e81-40be-a975-d481efbe70fb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>news_link</th>\n","      <th>outlet</th>\n","      <th>topic</th>\n","      <th>type</th>\n","      <th>Label_bias</th>\n","      <th>label_opinion</th>\n","      <th>biased_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Republican president assumed he was helpin...</td>\n","      <td>http://www.msnbc.com/rachel-maddow-show/auto-i...</td>\n","      <td>msnbc</td>\n","      <td>environment</td>\n","      <td>left</td>\n","      <td>Biased</td>\n","      <td>Expresses writer’s opinion</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Though the indictment of a woman for her own p...</td>\n","      <td>https://eu.usatoday.com/story/news/nation/2019...</td>\n","      <td>usa-today</td>\n","      <td>abortion</td>\n","      <td>center</td>\n","      <td>Non-biased</td>\n","      <td>Somewhat factual but also opinionated</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ingraham began the exchange by noting American...</td>\n","      <td>https://www.breitbart.com/economy/2020/01/12/d...</td>\n","      <td>breitbart</td>\n","      <td>immigration</td>\n","      <td>right</td>\n","      <td>No agreement</td>\n","      <td>No agreement</td>\n","      <td>['flood']</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The tragedy of America’s 18 years in Afghanist...</td>\n","      <td>http://feedproxy.google.com/~r/breitbart/~3/ER...</td>\n","      <td>breitbart</td>\n","      <td>international-politics-and-world-news</td>\n","      <td>right</td>\n","      <td>Biased</td>\n","      <td>Somewhat factual but also opinionated</td>\n","      <td>['tragedy', 'stubborn']</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The justices threw out a challenge from gun ri...</td>\n","      <td>https://www.huffpost.com/entry/supreme-court-g...</td>\n","      <td>msnbc</td>\n","      <td>gun-control</td>\n","      <td>left</td>\n","      <td>Non-biased</td>\n","      <td>Entirely factual</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8966680a-3e81-40be-a975-d481efbe70fb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8966680a-3e81-40be-a975-d481efbe70fb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8966680a-3e81-40be-a975-d481efbe70fb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# binarize classification problem\n","df_sg1 = df_sg1[df_sg1['Label_bias']!='No agreement']\n","df_sg1 = df_sg1[df_sg1['Label_bias'].isna()==False]\n","df_sg1.replace(to_replace='Biased', value=1, inplace=True)\n","df_sg1.replace(to_replace='Non-biased', value=0, inplace=True)\n","\n","df_sg2 = df_sg2[df_sg2['Label_bias']!='No agreement']\n","df_sg2.replace(to_replace='Biased', value=1, inplace=True)\n","df_sg2.replace(to_replace='Non-biased', value=0, inplace=True)\n","\n","# # test pipeline set\n","# df_sg1, exclude = train_test_split(df_sg1, test_size=0.95)\n","# df_sg2, exclude = train_test_split(df_sg2, test_size=0.8)"],"metadata":{"id":"nepDepaIogJr","executionInfo":{"status":"ok","timestamp":1668972278439,"user_tz":420,"elapsed":6,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Stratified k-Fold instance\n","skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"],"metadata":{"id":"ak5YGicMogOq","executionInfo":{"status":"ok","timestamp":1668976929784,"user_tz":420,"elapsed":84,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# helper functions called in skfold loop\n","\n","def pd_to_tf(df):\n","    \"\"\"convert a pandas dataframe into a tensorflow dataset\"\"\"\n","    target = df.pop('Label_bias')\n","    sentence = df.pop('sentence')\n","    return tf.data.Dataset.from_tensor_slices((sentence.values, target.values))\n","\n","def plot_graphs(history, metric):\n","    plt.plot(history.history[metric])\n","    plt.plot(history.history['val_'+metric], '')\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(metric)\n","    plt.legend([metric, 'val_'+metric])\n","    plt.show()\n","\n","def tokenize(df, model_name):\n","    \"\"\"convert a pandas dataframe into a tensorflow dataset and run hugging face's tokenizer on data\"\"\"\n","    df2 = df.copy(deep=False)\n","    target = df2.pop('Label_bias')\n","    sentence = df2.pop('sentence')\n","    \n","    if model_name=='bert':\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    elif model_name=='roberta':\n","        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","    elif model_name=='deberta':\n","        tokenizer = DebertaTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n","    elif model_name=='electra':\n","        tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n","\n","    train_encodings = tokenizer(\n","                        sentence.tolist(),                      \n","                        add_special_tokens = True, # add [CLS], [SEP]\n","                        truncation = True, # cut off at max length of the text that can go to BERT\n","                        padding = True, # add [PAD] tokens\n","                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n","              )\n","    \n","    dataset = tf.data.Dataset.from_tensor_slices(\n","        (dict(train_encodings), \n","         target.tolist()))\n","    \n","    # clear unused memory\n","    del(df2)\n","    del(target)\n","    del(sentence)\n","    del(tokenizer)\n","    del(train_encodings)\n","    gc.collect()\n","    \n","    return dataset"],"metadata":{"id":"yrKJNDIsogU1","executionInfo":{"status":"ok","timestamp":1668972281292,"user_tz":420,"elapsed":78,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def run_model_5fold(df_name, df_train, model_name, freeze_encoder=False, pretrained=False, plot=False, batch_size=32, epochs=10):\n","    \"\"\"\"function to run 5-fold cross validation for any provided model\"\"\"\n","\n","    # these variables will be needed for skfold to select indices\n","    Y = df_train['Label_bias']\n","    X = df_train['sentence']\n","\n","    # hyperparams\n","    BUFFER_SIZE = 10000\n","    BATCH_SIZE = batch_size\n","    k = 1\n","\n","    val_loss = []\n","    val_acc = []\n","    val_prec = []\n","    val_rec = []\n","    val_f1 = []\n","    val_f1_micro = []\n","    val_f1_wmacro = []\n","    \n","    if pretrained==True:\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","        \n","        if model_name=='bert':\n","            transfer_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n","            \n","        elif model_name=='roberta':\n","            transfer_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n","            \n","        elif model_name=='deberta':\n","            transfer_model = TFDebertaForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n","            \n","        transfer_model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","        transfer_model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/{model_name}_final_checkpoint_news_headlines_USA')\n","        trained_model_layer = transfer_model.get_layer(index=0).get_weights()\n","            \n","\n","    for train_index, val_index in skfold.split(X,Y):\n","        print('### Start fold {}'.format(k))\n","\n","        # split into train and validation set\n","        train_dataset = df_train.iloc[train_index]\n","        val_dataset = df_train.iloc[val_index]\n","\n","        # prepare data for transformer\n","        train_dataset = tokenize(train_dataset, model_name)\n","        val_dataset = tokenize(val_dataset, model_name)\n","\n","        # mini-batch it\n","        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n","        val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","        # create new model\n","        if model_name == 'bert':\n","            model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n","        elif model_name == 'roberta':\n","            model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n","        elif model_name == 'electra':\n","            model = TFElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator')\n","        elif model_name == 'deberta':\n","            model = TFDebertaForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n","\n","        # freeze flags whether encoder layer should be frozen to not destroy transfer learning. Only set to false when enough data is provided\n","        if freeze_encoder == True:\n","            for w in model.get_layer(index=0).weights:\n","                w._trainable = False\n","\n","        # compile it\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","        model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","\n","        # transfer learning\n","        if pretrained == True:\n","            model.get_layer(index=0).set_weights(trained_model_layer) # load bias-specific weights\n","\n","        # after 2 epochs without improvement, stop training\n","        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n","\n","        # fit it\n","        history = model.fit(train_dataset, epochs=epochs, validation_data = val_dataset, callbacks=[callback])\n","\n","        # plot history\n","        if plot:\n","            plot_graphs(history,'loss')\n","\n","        # evaluate\n","        loss = model.evaluate(val_dataset)\n","\n","        logits = model.predict(val_dataset)  \n","        yhats = []\n","        for i in logits[0]:\n","            # assign class label according to highest logit\n","            candidates = i.tolist()\n","            decision = candidates.index(max(candidates))\n","            yhats.append(decision)\n","\n","        y = []\n","        for text, label in val_dataset.unbatch():   \n","            y.append(label.numpy())\n","        \n","\n","        val_loss.append(loss)\n","        val_acc.append(accuracy_score(y, yhats))\n","        val_prec.append(precision_score(y, yhats))\n","        val_rec.append(recall_score(y, yhats))\n","        val_f1.append(f1_score(y, yhats))\n","        val_f1_micro.append(f1_score(y, yhats, average='micro'))\n","        val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n","        \n","        \n","\n","        tf.keras.backend.clear_session()\n","        \n","        # clear unused memory\n","        del(train_dataset)\n","        del(val_dataset)\n","        del(history)\n","        del(optimizer)\n","        del(callback)\n","        del(loss)\n","        del(y)\n","        gc.collect()\n","\n","        k += 1\n","    \n","    # save model weights\n","    trained_layer = model.get_layer(index=0).get_weights()\n","    model.save_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/{model_name}_{pretrained}_{df_name}_main')\n","\n","    return {'loss': val_loss, 'acc': val_acc, 'prec': val_prec, 'rec': val_rec, 'f1': val_f1, \n","            'f1_micro': val_f1_micro, 'f1_wmacro': val_f1_wmacro, 'model_name': model_name, \n","            'distant': pretrained, 'df_name': df_name} "],"metadata":{"id":"AWkzqZ0SpJMS","executionInfo":{"status":"ok","timestamp":1668976709337,"user_tz":420,"elapsed":103,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["def measure(d, results):\n","    loss_cv = np.mean(d['loss'])\n","    acc_cv = np.mean(d['acc'])\n","    prec_cv = np.mean(d['prec'])\n","    rec_cv = np.mean(d['rec'])\n","    f1_cv = np.mean(d['f1'])\n","    f1_micro_cv = np.mean(d['f1_micro'])\n","    f1_wmacro_cv = np.mean(d['f1_wmacro'])\n","    \n","    row = {\n","        'Dataset': d['df_name'],\n","        'Model': d['model_name'], \n","        'Distant': d['distant'], \n","        'Loss': loss_cv, \n","        'Accuracy': acc_cv, \n","        'Precision': prec_cv, \n","        'Recall': rec_cv, \n","        'F1': f1_cv, \n","        'F1 Micro': f1_micro_cv, \n","        'F1 Weighted': f1_wmacro_cv\n","    }\n","    \n","    results = results.append(row, ignore_index=True)\n","    \n","    return results"],"metadata":{"id":"-EDDV--apYa3","executionInfo":{"status":"ok","timestamp":1668976571761,"user_tz":420,"elapsed":73,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["## instantiate results df\n","columns = ['Dataset', 'Model', 'Distant', 'Loss', 'Accuracy', 'Precision', 'Recall', 'F1', 'F1 Micro', 'F1 Weighted']\n","results = pd.DataFrame(columns=columns)"],"metadata":{"id":"76O8GyPRpYlb","executionInfo":{"status":"ok","timestamp":1668976574484,"user_tz":420,"elapsed":76,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["# BERT"],"metadata":{"id":"-GFNtWBiqDg9"}},{"cell_type":"code","source":["# Sg1 \n","results = measure(run_model_5fold('sg1', df_sg1, 'bert', freeze_encoder=False, pretrained=False, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qq4uYHn6pYoq","executionInfo":{"status":"ok","timestamp":1668891301690,"user_tz":420,"elapsed":998800,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"be744450-cfa0-4d60-bb85-692e08fe7f15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 52s 908ms/step - loss: 0.6820 - val_loss: 0.6231\n","Epoch 2/10\n","39/39 [==============================] - 31s 804ms/step - loss: 0.6370 - val_loss: 0.5427\n","Epoch 3/10\n","39/39 [==============================] - 31s 801ms/step - loss: 0.4794 - val_loss: 0.7569\n","Epoch 4/10\n","39/39 [==============================] - 31s 806ms/step - loss: 0.5520 - val_loss: 0.5681\n","10/10 [==============================] - 2s 200ms/step - loss: 0.5427\n","10/10 [==============================] - 5s 195ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 51s 920ms/step - loss: 1.0844 - val_loss: 0.6939\n","Epoch 2/10\n","39/39 [==============================] - 32s 833ms/step - loss: 0.6492 - val_loss: 0.6145\n","Epoch 3/10\n","39/39 [==============================] - 32s 822ms/step - loss: 0.8369 - val_loss: 0.7789\n","Epoch 4/10\n","39/39 [==============================] - 32s 830ms/step - loss: 0.9948 - val_loss: 0.7029\n","10/10 [==============================] - 3s 258ms/step - loss: 0.6145\n","10/10 [==============================] - 6s 250ms/step\n","### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 50s 895ms/step - loss: 0.6915 - val_loss: 0.6266\n","Epoch 2/10\n","39/39 [==============================] - 32s 808ms/step - loss: 0.6123 - val_loss: 0.5787\n","Epoch 3/10\n","39/39 [==============================] - 31s 793ms/step - loss: 0.3721 - val_loss: 1.8531\n","Epoch 4/10\n","39/39 [==============================] - 31s 798ms/step - loss: 0.3620 - val_loss: 1.2932\n","10/10 [==============================] - 2s 195ms/step - loss: 0.5787\n","10/10 [==============================] - 6s 190ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 50s 898ms/step - loss: 0.7468 - val_loss: 0.8687\n","Epoch 2/10\n","39/39 [==============================] - 32s 814ms/step - loss: 0.6962 - val_loss: 0.6147\n","Epoch 3/10\n","39/39 [==============================] - 32s 812ms/step - loss: 0.5223 - val_loss: 0.5228\n","Epoch 4/10\n","39/39 [==============================] - 31s 805ms/step - loss: 0.6727 - val_loss: 0.6500\n","Epoch 5/10\n","39/39 [==============================] - 31s 806ms/step - loss: 0.4994 - val_loss: 0.8619\n","10/10 [==============================] - 2s 212ms/step - loss: 0.5228\n","10/10 [==============================] - 6s 208ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 51s 916ms/step - loss: 1.8748 - val_loss: 0.6302\n","Epoch 2/10\n","39/39 [==============================] - 32s 823ms/step - loss: 0.6072 - val_loss: 0.5268\n","Epoch 3/10\n","39/39 [==============================] - 32s 821ms/step - loss: 0.5049 - val_loss: 0.5215\n","Epoch 4/10\n","39/39 [==============================] - 31s 800ms/step - loss: 0.3730 - val_loss: 1.2530\n","Epoch 5/10\n","39/39 [==============================] - 31s 803ms/step - loss: 0.4301 - val_loss: 2.1705\n","10/10 [==============================] - 3s 254ms/step - loss: 0.5215\n","10/10 [==============================] - 6s 249ms/step\n","  Dataset Model Distant     Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  bert   False  0.55603  0.451406   0.400755  0.487284  0.420398   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.451406     0.413136  \n"]}]},{"cell_type":"code","source":["# Sg2 \n","results = measure(run_model_5fold('sg2', df_sg2, 'bert', freeze_encoder=False, pretrained=False, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9sX4F4mpYr_","executionInfo":{"status":"ok","timestamp":1668893429901,"user_tz":420,"elapsed":2128222,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"8348f1d6-a92c-4fa8-cf8d-3d52b3b5b0a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 103s 851ms/step - loss: 0.8114 - val_loss: 0.5954\n","Epoch 2/10\n","92/92 [==============================] - 74s 807ms/step - loss: 0.5850 - val_loss: 0.5808\n","Epoch 3/10\n","92/92 [==============================] - 74s 804ms/step - loss: 0.5578 - val_loss: 1.9449\n","Epoch 4/10\n","92/92 [==============================] - 74s 805ms/step - loss: 0.6981 - val_loss: 0.6031\n","23/23 [==============================] - 5s 214ms/step - loss: 0.5808\n","23/23 [==============================] - 8s 210ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 94s 853ms/step - loss: 4.1306 - val_loss: 4.0944\n","Epoch 2/10\n","92/92 [==============================] - 75s 811ms/step - loss: 4.0864 - val_loss: 4.0666\n","Epoch 3/10\n","92/92 [==============================] - 75s 810ms/step - loss: 4.0769 - val_loss: 4.0642\n","Epoch 4/10\n","92/92 [==============================] - 72s 782ms/step - loss: 6.6546 - val_loss: 7.5970\n","Epoch 5/10\n","92/92 [==============================] - 72s 779ms/step - loss: 7.6022 - val_loss: 7.5970\n","23/23 [==============================] - 5s 232ms/step - loss: 4.0642\n","23/23 [==============================] - 9s 228ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 94s 849ms/step - loss: 0.7139 - val_loss: 0.5838\n","Epoch 2/10\n","92/92 [==============================] - 75s 811ms/step - loss: 0.7023 - val_loss: 0.5615\n","Epoch 3/10\n","92/92 [==============================] - 74s 802ms/step - loss: 0.4800 - val_loss: 0.6918\n","Epoch 4/10\n","92/92 [==============================] - 73s 799ms/step - loss: 0.3972 - val_loss: 1.4325\n","23/23 [==============================] - 5s 216ms/step - loss: 0.5615\n","23/23 [==============================] - 8s 212ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 94s 857ms/step - loss: 0.9733 - val_loss: 0.6904\n","Epoch 2/10\n","92/92 [==============================] - 75s 815ms/step - loss: 0.7640 - val_loss: 0.8117\n","Epoch 3/10\n","92/92 [==============================] - 75s 815ms/step - loss: 0.7932 - val_loss: 0.6873\n","Epoch 4/10\n","92/92 [==============================] - 75s 817ms/step - loss: 0.7252 - val_loss: 0.6248\n","Epoch 5/10\n","92/92 [==============================] - 75s 818ms/step - loss: 0.6816 - val_loss: 0.5944\n","Epoch 6/10\n","92/92 [==============================] - 75s 815ms/step - loss: 0.5892 - val_loss: 0.7159\n","Epoch 7/10\n","92/92 [==============================] - 75s 814ms/step - loss: 1.7303 - val_loss: 4.1477\n","23/23 [==============================] - 6s 245ms/step - loss: 0.5944\n","23/23 [==============================] - 9s 238ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 90s 809ms/step - loss: 0.8279 - val_loss: 0.5310\n","Epoch 2/10\n","92/92 [==============================] - 71s 767ms/step - loss: 0.7823 - val_loss: 0.5282\n","Epoch 3/10\n","92/92 [==============================] - 71s 768ms/step - loss: 0.5862 - val_loss: 0.5254\n","Epoch 4/10\n","92/92 [==============================] - 70s 758ms/step - loss: 0.4830 - val_loss: 0.7231\n","Epoch 5/10\n","92/92 [==============================] - 70s 763ms/step - loss: 0.6393 - val_loss: 0.6875\n","23/23 [==============================] - 6s 266ms/step - loss: 0.5254\n","23/23 [==============================] - 10s 258ms/step\n","  Dataset Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  bert   False  0.556030  0.451406   0.400755  0.487284  0.420398   \n","1     sg2  bert   False  1.265255  0.520005   0.428627  0.239779  0.198883   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.451406     0.413136  \n","1  0.520005     0.378480  \n"]}]},{"cell_type":"markdown","source":["# RoBERTa"],"metadata":{"id":"jAQuBXO9qHYy"}},{"cell_type":"code","source":["# Sg1 \n","results = measure(run_model_5fold('sg1', df_sg1, 'roberta', freeze_encoder=False, pretrained=False, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0a9b8d3558bc46eda27648d8eed6610d","772a17563d5a4a278b63f0fb5956f71a","8f56816994794a71b25bc76ee1a788e7","3ee3d040cf98400683b9c3f99a891179","095b3e6e68414dc2a670c53344086262","43f8af40d16a496db390a4fec8b23878","4d6db122762f476187e2002f15b8e4ec","03981226032a448d823a91b3926c32d5","3bc636dc2fab4e8db80c96327cf789aa","859edd4cf2aa462ab5cbe50c069a1181","ea3d1024fb2546ed9f203a86a9492b68","532944fd01bf4466a02c5550902a64df","027f51d4c13f481d9ce094b723226ecd","053191a6d58344bf9653d8657f847040","a19db6a1337f4bbc822b696e238a8f11","85b538594133412c8150400656bc50bb","f42f2acdd1ce4274aa8a52a1ee67c008","551e1383a062485993f3d63114cf7774","40346bcad754468f89db8364811c6a95","e7d48c00e17640b28116b1fffc03baab","4983b3f0e826452ca11906d6d8c93d01","18a8678a989447f4a56f53c42fc9c798","ae30554a34f249518c6d4c1bb7609d10","14fea4162fb74ba69df492974b3dd36d","1de0bba3e23a45a5b804053308d5e473","9cf3b3858cf54252b4815cfeb733c628","b1eef2786e134653862d7804350d722a","49a77c7a86cb4cdab27fa08f17c9307d","429794215e524dd1a9d1c43f5c93a2f8","aee12247cc994ab9a7a408c5f6a7ca00","7059863d79a7488db133ef0a2c21988d","b6c674031e3641cdbb5eb610e8b26a73","e5a343e98b5c427fb03480a197829284","e5082245995d4a2493443cfefd5041ae","dee963d1224b4d19ad76fa254a9f99bc","960473289b7e4323b85f94bd4b42ed48","98810ab376874eedba4c0d7dae636bee","2b0878d76b2e471fbe96d6d9dab03a13","45da057a65954e5e8d89e9c2c009e8f5","e86671379b404531853ff9fc128715b7","ab53f6411abc48d199bf28e83c43afc3","9b20dafc73ff41e4a0c7175968ed078c","846358357c63405692183ccbeb6ae217","6da66d0b5dee47d9baa0f3ab19a30353"]},"id":"FDGsJiAeqCeZ","executionInfo":{"status":"ok","timestamp":1668894044946,"user_tz":420,"elapsed":277381,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"efab85f1-d614-48e8-cc28-b24820036bdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a9b8d3558bc46eda27648d8eed6610d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532944fd01bf4466a02c5550902a64df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae30554a34f249518c6d4c1bb7609d10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/657M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5082245995d4a2493443cfefd5041ae"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 29s 214ms/step - loss: 0.7345 - val_loss: 0.5399\n","Epoch 2/10\n","39/39 [==============================] - 4s 113ms/step - loss: 0.6572 - val_loss: 0.6137\n","Epoch 3/10\n","39/39 [==============================] - 5s 117ms/step - loss: 0.5933 - val_loss: 0.6018\n","10/10 [==============================] - 0s 35ms/step - loss: 0.5399\n","10/10 [==============================] - 3s 38ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 25s 208ms/step - loss: 2.0806 - val_loss: 0.7024\n","Epoch 2/10\n","39/39 [==============================] - 5s 122ms/step - loss: 0.7600 - val_loss: 0.6741\n","Epoch 3/10\n","39/39 [==============================] - 4s 115ms/step - loss: 0.7638 - val_loss: 1.3937\n","Epoch 4/10\n","39/39 [==============================] - 5s 119ms/step - loss: 0.8250 - val_loss: 0.7780\n","10/10 [==============================] - 0s 40ms/step - loss: 0.6741\n","10/10 [==============================] - 3s 42ms/step\n","### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 25s 207ms/step - loss: 1.3668 - val_loss: 0.8177\n","Epoch 2/10\n","39/39 [==============================] - 5s 121ms/step - loss: 0.7565 - val_loss: 0.6786\n","Epoch 3/10\n","39/39 [==============================] - 4s 113ms/step - loss: 0.6758 - val_loss: 0.6998\n","Epoch 4/10\n","39/39 [==============================] - 5s 118ms/step - loss: 0.6235 - val_loss: 1.1002\n","10/10 [==============================] - 0s 36ms/step - loss: 0.6786\n","10/10 [==============================] - 3s 38ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 25s 207ms/step - loss: 1.1117 - val_loss: 0.5658\n","Epoch 2/10\n","39/39 [==============================] - 5s 121ms/step - loss: 0.7099 - val_loss: 0.5633\n","Epoch 3/10\n","39/39 [==============================] - 4s 112ms/step - loss: 0.5624 - val_loss: 0.6306\n","Epoch 4/10\n","39/39 [==============================] - 5s 118ms/step - loss: 0.7880 - val_loss: 0.7066\n","10/10 [==============================] - 0s 35ms/step - loss: 0.5633\n","10/10 [==============================] - 3s 36ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 25s 209ms/step - loss: 0.8567 - val_loss: 0.5624\n","Epoch 2/10\n","39/39 [==============================] - 4s 115ms/step - loss: 0.7175 - val_loss: 0.6120\n","Epoch 3/10\n","39/39 [==============================] - 5s 123ms/step - loss: 0.6069 - val_loss: 0.4838\n","Epoch 4/10\n","39/39 [==============================] - 4s 114ms/step - loss: 0.5030 - val_loss: 0.5995\n","Epoch 5/10\n","39/39 [==============================] - 5s 122ms/step - loss: 0.4546 - val_loss: 0.4457\n","Epoch 6/10\n","39/39 [==============================] - 4s 115ms/step - loss: 0.4707 - val_loss: 1.4829\n","Epoch 7/10\n","39/39 [==============================] - 5s 120ms/step - loss: 0.3158 - val_loss: 2.0542\n","10/10 [==============================] - 0s 42ms/step - loss: 0.4457\n","10/10 [==============================] - 3s 43ms/step\n","  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538    0.56224  0.636251  0.460742   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n"]}]},{"cell_type":"code","source":["# Sg2 \n","results = measure(run_model_5fold('sg2', df_sg2, 'roberta', freeze_encoder=False, pretrained=False, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWAHW5PkqCju","executionInfo":{"status":"ok","timestamp":1668894504642,"user_tz":420,"elapsed":459708,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"934f3271-b4f1-4f64-c601-76e29d58e6b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 31s 154ms/step - loss: 0.7759 - val_loss: 0.5938\n","Epoch 2/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.6193 - val_loss: 0.5434\n","Epoch 3/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.5085 - val_loss: 0.5409\n","Epoch 4/10\n","92/92 [==============================] - 10s 114ms/step - loss: 0.5039 - val_loss: 0.6502\n","Epoch 5/10\n","92/92 [==============================] - 11s 116ms/step - loss: 0.5454 - val_loss: 0.6567\n","23/23 [==============================] - 1s 38ms/step - loss: 0.5409\n","23/23 [==============================] - 4s 40ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 31s 153ms/step - loss: 0.7901 - val_loss: 0.7018\n","Epoch 2/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.7182 - val_loss: 0.6636\n","Epoch 3/10\n","92/92 [==============================] - 10s 114ms/step - loss: 0.7071 - val_loss: 0.7612\n","Epoch 4/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.6478 - val_loss: 0.6100\n","Epoch 5/10\n","92/92 [==============================] - 10s 113ms/step - loss: 0.5719 - val_loss: 0.6369\n","Epoch 6/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.5555 - val_loss: 0.6095\n","Epoch 7/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.5858 - val_loss: 0.5945\n","Epoch 8/10\n","92/92 [==============================] - 10s 113ms/step - loss: 0.5188 - val_loss: 0.6091\n","Epoch 9/10\n","92/92 [==============================] - 11s 115ms/step - loss: 0.4684 - val_loss: 0.7080\n","23/23 [==============================] - 1s 37ms/step - loss: 0.5945\n","23/23 [==============================] - 4s 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 31s 153ms/step - loss: 0.8103 - val_loss: 0.5426\n","Epoch 2/10\n","92/92 [==============================] - 10s 113ms/step - loss: 0.6750 - val_loss: 0.5429\n","Epoch 3/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.5966 - val_loss: 0.4643\n","Epoch 4/10\n","92/92 [==============================] - 10s 113ms/step - loss: 0.7581 - val_loss: 0.6529\n","Epoch 5/10\n","92/92 [==============================] - 11s 115ms/step - loss: 0.7126 - val_loss: 0.7008\n","23/23 [==============================] - 1s 37ms/step - loss: 0.4643\n","23/23 [==============================] - 4s 37ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 31s 153ms/step - loss: 0.7457 - val_loss: 0.6952\n","Epoch 2/10\n","92/92 [==============================] - 11s 118ms/step - loss: 0.6403 - val_loss: 0.6022\n","Epoch 3/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.5368 - val_loss: 0.5399\n","Epoch 4/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.5920 - val_loss: 0.4837\n","Epoch 5/10\n","92/92 [==============================] - 10s 114ms/step - loss: 0.5621 - val_loss: 0.5652\n","Epoch 6/10\n","92/92 [==============================] - 11s 116ms/step - loss: 0.4524 - val_loss: 1.2246\n","23/23 [==============================] - 1s 38ms/step - loss: 0.4837\n","23/23 [==============================] - 4s 38ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 30s 142ms/step - loss: 0.7377 - val_loss: 0.4795\n","Epoch 2/10\n","92/92 [==============================] - 9s 102ms/step - loss: 0.7714 - val_loss: 0.6936\n","Epoch 3/10\n","92/92 [==============================] - 10s 105ms/step - loss: 0.7064 - val_loss: 0.6936\n","23/23 [==============================] - 1s 41ms/step - loss: 0.4795\n","23/23 [==============================] - 4s 41ms/step\n","  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538   0.562240  0.636251  0.460742   \n","1     sg2  roberta   False  0.512574  0.545579   0.522718  0.595028  0.418805   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n","1  0.545579     0.419941  \n"]}]},{"cell_type":"markdown","source":["# DeBERTa"],"metadata":{"id":"c7fSNiTKqNRI"}},{"cell_type":"code","source":["# Sg1 \n","results = measure(run_model_5fold('sg1', df_sg1, 'deberta', freeze_encoder=False, pretrained=False, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["62d068efc27d4206ba2613cd2e57868b","47d8c7d851ea429d89c0642a0c0e7b51","094552b53b524060a5abc321c2106ed5","469b9a6a06df49788dc4a6214022e9bc","872c292836024bbb93750aa98e6572fb","7d264455d09b4a638418e5a11a7d43fc","e9410d8174a34e7c9448f5c1292d88d3","fcd5fd2cb1124af0bfb382a6de78d732","6252306613ae4480b2fa4004f3d1a1b9","bef339c773d84e8885fc2be8b443ea66","85e87e67c3b64c29891a8be64bf28935","a04b5dd4737b4918891429fe8a13a3ec","d4a1915ca5f742f1aaa6b0d11c6edd8a","87257dcb104e4ddeba679370d3ed4e9b","e35073a346fd40e795ad93d6de8ca1b1","74fb3860c92c44689e64acef5662190a","f4fafe3fbf8e4936960c82ea9611ca04","ed4ae93eee584e29bfd82363e62a308a","0f6bdbf84def4c85bb1753c4cd9ac7b2","76946672010a4d6eac6dd8a92aba4446","024fea91f71d44c2bf491d4f7c88ccca","424bdad8c00448c8a68fcf2fff1caa00","101b545463824ad899ce947536da6e42","4b0a638955aa4757aa0398a3b7ad2b19","55b9ac7cabb84da4a95a1034cab73256","35c4b54bc9324eaabe49590ef3997ca7","9726116f7bf14ab886e0f2e37b7a7c60","fb1a537ada154ddd973b81a876e5ef66","37133930c2c545fb826287f03344d9d3","9e027605e1e94fc385984bac22727698","aa144279e8d3423199ebaaae154572a1","adf29755acd440e79048feb8ca59f4c6","b2704c485fcc4d70b04ea9d7ea682775","f24c9ac19c28460eb81e956e1c7f3c0a","3cbd8bef48be4542a79a6fba273eb85e","4374521eb0f34620ab49a812aa23ef3e","dddf2d34cbae4958a06b122dfafea30f","eb66b0febd3e4bf08e9c9fe6f7fecc73","bf84f370b462424abe460efd7fd24f83","b3005df58ee74187a0ad92b5545f6b57","9f2dc197e90f42509cbf38e3a13b088b","bf1e566b0eda4b37b5a373f7604e66b5","3331c82556bf4f68b5c66a48b89c3bd7","8341cb6947f14dcdac638d0eb0f7c078","121cde92b9034e07be1a8c02706f4dd5","e0467a7154a34c4e936c5e8e62396d38","cb69b906fb684c40815d7e35b552c546","48943b6ceda6461490467cfddf994f37","e2226c4e87da4d9bb26565d07412fa1d","a2308803984b486fa93be5907f4fe6f2","97f53a47b69e41639ab5f59fcf2361be","fe2c7f531f624c548ed2b1ee864e71df","80cbee57f3d34e0f9320fb72f0b406d6","fe74522a83f24df199136add8daa966b","42e617f3e6b3416fa9aa625bd74ab1ef"]},"id":"s68oPshrqeAe","executionInfo":{"status":"ok","timestamp":1668895082507,"user_tz":420,"elapsed":577876,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"8daa3aae-dec7-4be1-e54f-e02dfb644dfe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d068efc27d4206ba2613cd2e57868b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a04b5dd4737b4918891429fe8a13a3ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"101b545463824ad899ce947536da6e42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/744 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24c9ac19c28460eb81e956e1c7f3c0a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/555M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121cde92b9034e07be1a8c02706f4dd5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_tf_deberta.py:123: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n","Instructions for updating:\n","The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:93: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n","Instructions for updating:\n","The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"]},{"output_type":"stream","name":"stdout","text":["39/39 [==============================] - 57s 503ms/step - loss: 1.0929 - val_loss: 0.6967\n","Epoch 2/10\n","39/39 [==============================] - 13s 329ms/step - loss: 0.7154 - val_loss: 0.6939\n","Epoch 3/10\n","39/39 [==============================] - 13s 327ms/step - loss: 0.6948 - val_loss: 0.6804\n","Epoch 4/10\n","39/39 [==============================] - 12s 319ms/step - loss: 0.7384 - val_loss: 0.7273\n","Epoch 5/10\n","39/39 [==============================] - 13s 324ms/step - loss: 0.7219 - val_loss: 0.6938\n","10/10 [==============================] - 0s 44ms/step - loss: 0.6804\n","10/10 [==============================] - 7s 45ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 55s 508ms/step - loss: 3.7388 - val_loss: 0.8424\n","Epoch 2/10\n","39/39 [==============================] - 13s 331ms/step - loss: 0.9312 - val_loss: 0.6925\n","Epoch 3/10\n","39/39 [==============================] - 12s 320ms/step - loss: 0.7164 - val_loss: 1.0949\n","Epoch 4/10\n","39/39 [==============================] - 13s 326ms/step - loss: 3.3474 - val_loss: 4.2554\n","10/10 [==============================] - 1s 51ms/step - loss: 0.6925\n","10/10 [==============================] - 7s 52ms/step\n","### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 55s 506ms/step - loss: 3.5648 - val_loss: 4.0466\n","Epoch 2/10\n","39/39 [==============================] - 13s 329ms/step - loss: 4.1658 - val_loss: 4.0408\n","Epoch 3/10\n","39/39 [==============================] - 13s 327ms/step - loss: 5.1794 - val_loss: 2.8801\n","Epoch 4/10\n","39/39 [==============================] - 12s 320ms/step - loss: 1.2994 - val_loss: 4.0375\n","Epoch 5/10\n","39/39 [==============================] - 13s 324ms/step - loss: 5.8552 - val_loss: 7.4379\n","10/10 [==============================] - 0s 44ms/step - loss: 2.8801\n","10/10 [==============================] - 7s 46ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 56s 506ms/step - loss: 2.0577 - val_loss: 0.6662\n","Epoch 2/10\n","39/39 [==============================] - 12s 319ms/step - loss: 0.7593 - val_loss: 0.8156\n","Epoch 3/10\n","39/39 [==============================] - 13s 324ms/step - loss: 3.7703 - val_loss: 4.2662\n","10/10 [==============================] - 0s 42ms/step - loss: 0.6662\n","10/10 [==============================] - 7s 43ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 56s 506ms/step - loss: 3.9307 - val_loss: 4.0217\n","Epoch 2/10\n","39/39 [==============================] - 12s 315ms/step - loss: 4.0521 - val_loss: 4.6274\n","Epoch 3/10\n","39/39 [==============================] - 13s 322ms/step - loss: 4.1095 - val_loss: 4.0139\n","Epoch 4/10\n","39/39 [==============================] - 12s 313ms/step - loss: 4.0296 - val_loss: 4.0262\n","Epoch 5/10\n","39/39 [==============================] - 12s 319ms/step - loss: 5.8768 - val_loss: 6.2117\n","10/10 [==============================] - 1s 52ms/step - loss: 4.0139\n","10/10 [==============================] - 7s 53ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538   0.562240  0.636251  0.460742   \n","1     sg2  roberta   False  0.512574  0.545579   0.522718  0.595028  0.418805   \n","2     sg1  deberta   False  1.786612  0.513528   0.431187  0.633163  0.467586   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n","1  0.545579     0.419941  \n","2  0.513528     0.389934  \n"]}]},{"cell_type":"code","source":["# Sg2\n","results = measure(run_model_5fold('sg2', df_sg2, 'deberta', freeze_encoder=False, pretrained=False, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5m4YUMoqeJ2","executionInfo":{"status":"ok","timestamp":1668896237941,"user_tz":420,"elapsed":1155447,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"f079c968-f76e-4435-adbe-b7b2bc0638b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 72s 398ms/step - loss: 2.5788 - val_loss: 4.1659\n","Epoch 2/10\n","92/92 [==============================] - 30s 321ms/step - loss: 5.0330 - val_loss: 7.6679\n","Epoch 3/10\n","92/92 [==============================] - 30s 322ms/step - loss: 7.6679 - val_loss: 7.6679\n","23/23 [==============================] - 1s 46ms/step - loss: 4.1659\n","23/23 [==============================] - 8s 47ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 73s 400ms/step - loss: 4.0199 - val_loss: 4.0762\n","Epoch 2/10\n","92/92 [==============================] - 30s 326ms/step - loss: 4.1358 - val_loss: 4.0649\n","Epoch 3/10\n","92/92 [==============================] - 30s 321ms/step - loss: 4.1109 - val_loss: 4.3380\n","Epoch 4/10\n","92/92 [==============================] - 30s 322ms/step - loss: 6.3343 - val_loss: 7.6679\n","23/23 [==============================] - 1s 44ms/step - loss: 4.0649\n","23/23 [==============================] - 7s 44ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 73s 399ms/step - loss: 4.2810 - val_loss: 4.3428\n","Epoch 2/10\n","92/92 [==============================] - 30s 324ms/step - loss: 3.4587 - val_loss: 4.1608\n","Epoch 3/10\n","92/92 [==============================] - 30s 324ms/step - loss: 4.1929 - val_loss: 4.1028\n","Epoch 4/10\n","92/92 [==============================] - 30s 321ms/step - loss: 4.0789 - val_loss: 4.5319\n","Epoch 5/10\n","92/92 [==============================] - 30s 324ms/step - loss: 4.0917 - val_loss: 4.1002\n","Epoch 6/10\n","92/92 [==============================] - 30s 325ms/step - loss: 2.1333 - val_loss: 1.6625\n","Epoch 7/10\n","92/92 [==============================] - 30s 324ms/step - loss: 0.8000 - val_loss: 0.6963\n","Epoch 8/10\n","92/92 [==============================] - 30s 324ms/step - loss: 0.7117 - val_loss: 0.6930\n","Epoch 9/10\n","92/92 [==============================] - 29s 320ms/step - loss: 0.7046 - val_loss: 0.6935\n","Epoch 10/10\n","92/92 [==============================] - 30s 322ms/step - loss: 0.7004 - val_loss: 0.6932\n","23/23 [==============================] - 1s 45ms/step - loss: 0.6930\n","23/23 [==============================] - 7s 45ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 73s 400ms/step - loss: 0.9726 - val_loss: 0.6936\n","Epoch 2/10\n","92/92 [==============================] - 30s 321ms/step - loss: 3.0709 - val_loss: 4.1147\n","Epoch 3/10\n","92/92 [==============================] - 30s 323ms/step - loss: 4.1481 - val_loss: 4.1951\n","23/23 [==============================] - 1s 46ms/step - loss: 0.6936\n","23/23 [==============================] - 7s 46ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 69s 355ms/step - loss: 4.7392 - val_loss: 5.8658\n","Epoch 2/10\n","92/92 [==============================] - 26s 280ms/step - loss: 4.2130 - val_loss: 4.0721\n","Epoch 3/10\n","92/92 [==============================] - 26s 279ms/step - loss: 4.1448 - val_loss: 4.0514\n","Epoch 4/10\n","92/92 [==============================] - 26s 279ms/step - loss: 3.7108 - val_loss: 0.7227\n","Epoch 5/10\n","92/92 [==============================] - 25s 275ms/step - loss: 1.0382 - val_loss: 0.7329\n","Epoch 6/10\n","92/92 [==============================] - 26s 279ms/step - loss: 0.7069 - val_loss: 0.6945\n","Epoch 7/10\n","92/92 [==============================] - 25s 275ms/step - loss: 0.6991 - val_loss: 0.6966\n","Epoch 8/10\n","92/92 [==============================] - 26s 279ms/step - loss: 0.6993 - val_loss: 0.6931\n","Epoch 9/10\n","92/92 [==============================] - 25s 275ms/step - loss: 0.7037 - val_loss: 0.6931\n","Epoch 10/10\n","92/92 [==============================] - 26s 278ms/step - loss: 0.6980 - val_loss: 0.6937\n","23/23 [==============================] - 1s 53ms/step - loss: 0.6931\n","23/23 [==============================] - 8s 53ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538   0.562240  0.636251  0.460742   \n","1     sg2  roberta   False  0.512574  0.545579   0.522718  0.595028  0.418805   \n","2     sg1  deberta   False  1.786612  0.513528   0.431187  0.633163  0.467586   \n","3     sg2  deberta   False  2.062078  0.500138   0.296457  0.600000  0.396837   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n","1  0.545579     0.419941  \n","2  0.513528     0.389934  \n","3  0.500138     0.335278  \n"]}]},{"cell_type":"markdown","source":["# BERT w/ Distant"],"metadata":{"id":"w9GJ1MLwqgtQ"}},{"cell_type":"code","source":["# Sg1 \n","results = measure(run_model_5fold('sg1', df_sg1, 'bert', freeze_encoder=False, pretrained=True, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["415bf4e128cb48b7a026f6d5b6cc2dad","cd17fb25426045b5b1dd49df73211cff","080e90a57fcf43c59e48d3cf670835be","7926399739ed45f18b239d169fd1903f","762cf7566f2d48d78a8e56b165c2bdab","ba694602ca3d41469d574c01ff35f7b2","594dad76ae234b2fb5638cf3e811b25d","34129e37b8c349e08706f7b973d46c10","f8d815ec90ee4f9696152ec57044f99b","ea3792d9933446bfb3a4108dee03dfaf","6e13352920d84ba480e7809cbe607642","c54a21945b8c4b4488c7c4e2a6f2c15c","4d66293f93c04c418a6718341a1b3bbf","146d8d6ea27a42c3af466e63415dd869","e557b41c02f54122bf86203b4c275b25","99e9001e227c4b2890b414c5a5a801cc","03348d8d75664f63bbda278155067d02","67aa05b103a94b03a9b3e7765855ecae","5a93b1cabfe14837bab746eca0971a8b","31f7e13cf6604de584edc616930418e8","8f4040edd6c34717b3d9088f137297bd","2cde9e52c065401788a4d54bfa64cdda"]},"id":"OzCZmELQqkS2","executionInfo":{"status":"ok","timestamp":1668899270363,"user_tz":420,"elapsed":275892,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"45a72e81-e1bd-4396-df95-cb208702f3cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"415bf4e128cb48b7a026f6d5b6cc2dad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c54a21945b8c4b4488c7c4e2a6f2c15c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 30s 229ms/step - loss: 3.0657 - val_loss: 0.7009\n","Epoch 2/10\n","39/39 [==============================] - 5s 121ms/step - loss: 0.6750 - val_loss: 0.6117\n","Epoch 3/10\n","39/39 [==============================] - 5s 119ms/step - loss: 0.5728 - val_loss: 0.5942\n","Epoch 4/10\n","39/39 [==============================] - 4s 110ms/step - loss: 0.4304 - val_loss: 0.8473\n","Epoch 5/10\n","39/39 [==============================] - 4s 113ms/step - loss: 0.5414 - val_loss: 1.0412\n","10/10 [==============================] - 0s 36ms/step - loss: 0.5942\n","10/10 [==============================] - 3s 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 24s 204ms/step - loss: 0.9196 - val_loss: 0.7094\n","Epoch 2/10\n","39/39 [==============================] - 4s 111ms/step - loss: 0.6157 - val_loss: 0.7694\n","Epoch 3/10\n","39/39 [==============================] - 5s 119ms/step - loss: 0.5521 - val_loss: 0.5281\n","Epoch 4/10\n","39/39 [==============================] - 4s 110ms/step - loss: 0.5386 - val_loss: 0.7836\n","Epoch 5/10\n","39/39 [==============================] - 4s 114ms/step - loss: 0.3601 - val_loss: 1.6887\n","10/10 [==============================] - 0s 41ms/step - loss: 0.5281\n","10/10 [==============================] - 4s 42ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 24s 204ms/step - loss: 2.6026 - val_loss: 0.6948\n","Epoch 2/10\n","39/39 [==============================] - 5s 118ms/step - loss: 0.6735 - val_loss: 0.6879\n","Epoch 3/10\n","39/39 [==============================] - 5s 116ms/step - loss: 0.7189 - val_loss: 0.6718\n","Epoch 4/10\n","39/39 [==============================] - 5s 118ms/step - loss: 0.5662 - val_loss: 0.5824\n","Epoch 5/10\n","39/39 [==============================] - 4s 109ms/step - loss: 0.4242 - val_loss: 0.9335\n","Epoch 6/10\n","39/39 [==============================] - 4s 113ms/step - loss: 0.2936 - val_loss: 1.7310\n","10/10 [==============================] - 0s 35ms/step - loss: 0.5824\n","10/10 [==============================] - 4s 36ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 24s 204ms/step - loss: 2.4129 - val_loss: 0.6518\n","Epoch 2/10\n","39/39 [==============================] - 5s 118ms/step - loss: 0.5884 - val_loss: 0.5706\n","Epoch 3/10\n","39/39 [==============================] - 4s 109ms/step - loss: 0.4911 - val_loss: 0.6041\n","Epoch 4/10\n","39/39 [==============================] - 4s 113ms/step - loss: 0.5406 - val_loss: 0.8565\n","10/10 [==============================] - 0s 36ms/step - loss: 0.5706\n","10/10 [==============================] - 4s 37ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 24s 204ms/step - loss: 4.1121 - val_loss: 4.0460\n","Epoch 2/10\n","39/39 [==============================] - 5s 120ms/step - loss: 4.0494 - val_loss: 3.9925\n","Epoch 3/10\n","39/39 [==============================] - 4s 111ms/step - loss: 3.9971 - val_loss: 4.0068\n","Epoch 4/10\n","39/39 [==============================] - 4s 115ms/step - loss: 3.9212 - val_loss: 3.9938\n","10/10 [==============================] - 0s 40ms/step - loss: 3.9925\n","10/10 [==============================] - 4s 42ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"]},{"output_type":"stream","name":"stdout","text":["  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538   0.562240  0.636251  0.460742   \n","1     sg2  roberta   False  0.512574  0.545579   0.522718  0.595028  0.418805   \n","2     sg1  deberta   False  1.786612  0.513528   0.431187  0.633163  0.467586   \n","3     sg2  deberta   False  2.062078  0.500138   0.296457  0.600000  0.396837   \n","4     sg1  electra   False  0.550895  0.475357   0.444049  0.380805  0.393093   \n","5     sg2  electra   False  0.482666  0.506686   0.651247  0.436464  0.394047   \n","6     sg1  deberta    True  0.617424  0.492855   0.334588  0.584009  0.418621   \n","7     sg2  deberta    True  0.620772  0.412249   0.402475  0.313812  0.241562   \n","8     sg1     bert    True  1.253572  0.452740   0.246955  0.457718  0.316711   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n","1  0.545579     0.419941  \n","2  0.513528     0.389934  \n","3  0.500138     0.335278  \n","4  0.475357     0.448213  \n","5  0.506686     0.440985  \n","6  0.492855     0.396586  \n","7  0.412249     0.310578  \n","8  0.452740     0.320406  \n"]}]},{"cell_type":"code","source":["# Sg2 \n","results = measure(run_model_5fold('sg2', df_sg2, 'bert', freeze_encoder=False, pretrained=True, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qH36nm7qkzb","executionInfo":{"status":"ok","timestamp":1668899644928,"user_tz":420,"elapsed":374580,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"dbcbf069-7573-42e6-a4a2-c3995ba39353"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 31s 150ms/step - loss: 1.1266 - val_loss: 0.6763\n","Epoch 2/10\n","92/92 [==============================] - 10s 114ms/step - loss: 0.6339 - val_loss: 0.5299\n","Epoch 3/10\n","92/92 [==============================] - 10s 109ms/step - loss: 0.5597 - val_loss: 0.6219\n","Epoch 4/10\n","92/92 [==============================] - 10s 111ms/step - loss: 0.4872 - val_loss: 0.7521\n","23/23 [==============================] - 1s 37ms/step - loss: 0.5299\n","23/23 [==============================] - 4s 39ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 30s 149ms/step - loss: 0.9664 - val_loss: 0.6089\n","Epoch 2/10\n","92/92 [==============================] - 10s 113ms/step - loss: 0.5612 - val_loss: 0.5037\n","Epoch 3/10\n","92/92 [==============================] - 10s 109ms/step - loss: 0.6009 - val_loss: 0.6201\n","Epoch 4/10\n","92/92 [==============================] - 10s 111ms/step - loss: 0.5387 - val_loss: 0.8742\n","23/23 [==============================] - 1s 38ms/step - loss: 0.5037\n","23/23 [==============================] - 5s 39ms/step\n","### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 30s 149ms/step - loss: 0.9103 - val_loss: 0.6702\n","Epoch 2/10\n","92/92 [==============================] - 10s 113ms/step - loss: 0.6088 - val_loss: 0.5095\n","Epoch 3/10\n","92/92 [==============================] - 10s 110ms/step - loss: 0.5307 - val_loss: 1.3285\n","Epoch 4/10\n","92/92 [==============================] - 10s 111ms/step - loss: 0.8885 - val_loss: 0.6245\n","23/23 [==============================] - 1s 37ms/step - loss: 0.5095\n","23/23 [==============================] - 5s 38ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 30s 149ms/step - loss: 0.7249 - val_loss: 0.5362\n","Epoch 2/10\n","92/92 [==============================] - 10s 110ms/step - loss: 0.5609 - val_loss: 0.9209\n","Epoch 3/10\n","92/92 [==============================] - 10s 112ms/step - loss: 0.4462 - val_loss: 0.6879\n","23/23 [==============================] - 1s 39ms/step - loss: 0.5362\n","23/23 [==============================] - 5s 40ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 30s 143ms/step - loss: 2.0889 - val_loss: 0.6000\n","Epoch 2/10\n","92/92 [==============================] - 10s 107ms/step - loss: 0.5174 - val_loss: 0.5818\n","Epoch 3/10\n","92/92 [==============================] - 9s 103ms/step - loss: 0.4599 - val_loss: 1.8586\n","Epoch 4/10\n","92/92 [==============================] - 10s 106ms/step - loss: 0.3427 - val_loss: 1.9612\n","23/23 [==============================] - 1s 41ms/step - loss: 0.5818\n","23/23 [==============================] - 5s 41ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"]},{"output_type":"stream","name":"stdout","text":["  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538   0.562240  0.636251  0.460742   \n","1     sg2  roberta   False  0.512574  0.545579   0.522718  0.595028  0.418805   \n","2     sg1  deberta   False  1.786612  0.513528   0.431187  0.633163  0.467586   \n","3     sg2  deberta   False  2.062078  0.500138   0.296457  0.600000  0.396837   \n","4     sg1  electra   False  0.550895  0.475357   0.444049  0.380805  0.393093   \n","5     sg2  electra   False  0.482666  0.506686   0.651247  0.436464  0.394047   \n","6     sg1  deberta    True  0.617424  0.492855   0.334588  0.584009  0.418621   \n","7     sg2  deberta    True  0.620772  0.412249   0.402475  0.313812  0.241562   \n","8     sg1     bert    True  1.253572  0.452740   0.246955  0.457718  0.316711   \n","9     sg2     bert    True  0.532223  0.558989   0.539072  0.743646  0.606827   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n","1  0.545579     0.419941  \n","2  0.513528     0.389934  \n","3  0.500138     0.335278  \n","4  0.475357     0.448213  \n","5  0.506686     0.440985  \n","6  0.492855     0.396586  \n","7  0.412249     0.310578  \n","8  0.452740     0.320406  \n","9  0.558989     0.502989  \n"]}]},{"cell_type":"markdown","source":["# RoBERTa w/ Distant"],"metadata":{"id":"FGtEvw5DqlU6"}},{"cell_type":"code","source":["# Sg1 \n","results = measure(run_model_5fold('sg1', df_sg1, 'roberta', freeze_encoder=False, pretrained=True, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmwIP7MlqpvA","executionInfo":{"status":"ok","timestamp":1668899916485,"user_tz":420,"elapsed":271567,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"11c30079-63e7-4c41-b4c4-2e143e5e04b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 25s 210ms/step - loss: 0.7682 - val_loss: 0.5338\n","Epoch 2/10\n","39/39 [==============================] - 5s 124ms/step - loss: 0.5815 - val_loss: 0.4845\n","Epoch 3/10\n","39/39 [==============================] - 4s 113ms/step - loss: 0.7735 - val_loss: 0.5143\n","Epoch 4/10\n","39/39 [==============================] - 5s 118ms/step - loss: 0.8105 - val_loss: 0.6985\n","10/10 [==============================] - 0s 36ms/step - loss: 0.4845\n","10/10 [==============================] - 3s 38ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 24s 213ms/step - loss: 0.8371 - val_loss: 0.6170\n","Epoch 2/10\n","39/39 [==============================] - 5s 124ms/step - loss: 0.6276 - val_loss: 0.5907\n","Epoch 3/10\n","39/39 [==============================] - 4s 115ms/step - loss: 0.5354 - val_loss: 1.1037\n","Epoch 4/10\n","39/39 [==============================] - 5s 120ms/step - loss: 0.4457 - val_loss: 0.8987\n","10/10 [==============================] - 0s 40ms/step - loss: 0.5907\n","10/10 [==============================] - 4s 42ms/step\n","### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 25s 211ms/step - loss: 0.8866 - val_loss: 0.5791\n","Epoch 2/10\n","39/39 [==============================] - 4s 114ms/step - loss: 0.6544 - val_loss: 0.5833\n","Epoch 3/10\n","39/39 [==============================] - 5s 119ms/step - loss: 0.6528 - val_loss: 0.6114\n","10/10 [==============================] - 0s 37ms/step - loss: 0.5791\n","10/10 [==============================] - 4s 38ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 25s 212ms/step - loss: 0.9474 - val_loss: 0.6919\n","Epoch 2/10\n","39/39 [==============================] - 5s 123ms/step - loss: 0.6788 - val_loss: 0.6088\n","Epoch 3/10\n","39/39 [==============================] - 4s 113ms/step - loss: 0.7213 - val_loss: 0.6967\n","Epoch 4/10\n","39/39 [==============================] - 5s 118ms/step - loss: 0.7086 - val_loss: 0.6943\n","10/10 [==============================] - 0s 36ms/step - loss: 0.6088\n","10/10 [==============================] - 4s 37ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 25s 211ms/step - loss: 1.6020 - val_loss: 0.5812\n","Epoch 2/10\n","39/39 [==============================] - 4s 114ms/step - loss: 0.6505 - val_loss: 0.5891\n","Epoch 3/10\n","39/39 [==============================] - 5s 123ms/step - loss: 0.6213 - val_loss: 0.5082\n","Epoch 4/10\n","39/39 [==============================] - 5s 122ms/step - loss: 0.5875 - val_loss: 0.4699\n","Epoch 5/10\n","39/39 [==============================] - 4s 114ms/step - loss: 0.4281 - val_loss: 0.7031\n","Epoch 6/10\n","39/39 [==============================] - 5s 119ms/step - loss: 0.7121 - val_loss: 0.6729\n","10/10 [==============================] - 0s 41ms/step - loss: 0.4699\n","10/10 [==============================] - 4s 43ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"]},{"output_type":"stream","name":"stdout","text":["   Dataset    Model Distant      Loss  Accuracy  Precision    Recall  \\\n","0      sg1  roberta   False  0.580312  0.515538   0.562240  0.636251   \n","1      sg2  roberta   False  0.512574  0.545579   0.522718  0.595028   \n","2      sg1  deberta   False  1.786612  0.513528   0.431187  0.633163   \n","3      sg2  deberta   False  2.062078  0.500138   0.296457  0.600000   \n","4      sg1  electra   False  0.550895  0.475357   0.444049  0.380805   \n","5      sg2  electra   False  0.482666  0.506686   0.651247  0.436464   \n","6      sg1  deberta    True  0.617424  0.492855   0.334588  0.584009   \n","7      sg2  deberta    True  0.620772  0.412249   0.402475  0.313812   \n","8      sg1     bert    True  1.253572  0.452740   0.246955  0.457718   \n","9      sg2     bert    True  0.532223  0.558989   0.539072  0.743646   \n","10     sg1  roberta    True  0.546565  0.468920   0.549649  0.386568   \n","\n","          F1  F1 Micro  F1 Weighted  \n","0   0.460742  0.515538     0.377869  \n","1   0.418805  0.545579     0.419941  \n","2   0.467586  0.513528     0.389934  \n","3   0.396837  0.500138     0.335278  \n","4   0.393093  0.475357     0.448213  \n","5   0.394047  0.506686     0.440985  \n","6   0.418621  0.492855     0.396586  \n","7   0.241562  0.412249     0.310578  \n","8   0.316711  0.452740     0.320406  \n","9   0.606827  0.558989     0.502989  \n","10  0.330550  0.468920     0.378031  \n"]}]},{"cell_type":"code","source":["# Sg2\n","results = measure(run_model_5fold('sg2', df_sg2, 'roberta', freeze_encoder=False, pretrained=True, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4l7vQxiLqqJR","executionInfo":{"status":"ok","timestamp":1668900331049,"user_tz":420,"elapsed":414577,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"06a857a3-a7bd-41de-d5bc-7a7508d5916e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 32s 155ms/step - loss: 0.7223 - val_loss: 0.4452\n","Epoch 2/10\n","92/92 [==============================] - 11s 115ms/step - loss: 0.5301 - val_loss: 0.4863\n","Epoch 3/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.5756 - val_loss: 0.4842\n","23/23 [==============================] - 1s 39ms/step - loss: 0.4452\n","23/23 [==============================] - 4s 38ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 30s 154ms/step - loss: 0.7716 - val_loss: 0.6925\n","Epoch 2/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.6886 - val_loss: 0.6360\n","Epoch 3/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.6454 - val_loss: 0.6182\n","Epoch 4/10\n","92/92 [==============================] - 10s 113ms/step - loss: 0.6797 - val_loss: 0.6842\n","Epoch 5/10\n","92/92 [==============================] - 11s 117ms/step - loss: 0.6015 - val_loss: 0.5252\n","Epoch 6/10\n","92/92 [==============================] - 10s 114ms/step - loss: 0.7711 - val_loss: 5.2449\n","Epoch 7/10\n","92/92 [==============================] - 11s 116ms/step - loss: 0.7240 - val_loss: 0.5291\n","23/23 [==============================] - 1s 38ms/step - loss: 0.5252\n","23/23 [==============================] - 5s 38ms/step\n","### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 31s 154ms/step - loss: 0.7718 - val_loss: 0.6509\n","Epoch 2/10\n","92/92 [==============================] - 11s 118ms/step - loss: 0.6984 - val_loss: 0.5172\n","Epoch 3/10\n","92/92 [==============================] - 11s 118ms/step - loss: 0.5759 - val_loss: 0.4751\n","Epoch 4/10\n","92/92 [==============================] - 11s 114ms/step - loss: 0.6130 - val_loss: 0.5323\n","Epoch 5/10\n","92/92 [==============================] - 11s 116ms/step - loss: 0.6582 - val_loss: 0.7095\n","23/23 [==============================] - 1s 38ms/step - loss: 0.4751\n","23/23 [==============================] - 5s 39ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 31s 155ms/step - loss: 1.2645 - val_loss: 0.6738\n","Epoch 2/10\n","92/92 [==============================] - 11s 118ms/step - loss: 0.6424 - val_loss: 0.5209\n","Epoch 3/10\n","92/92 [==============================] - 10s 114ms/step - loss: 0.5974 - val_loss: 1.3280\n","Epoch 4/10\n","92/92 [==============================] - 11s 116ms/step - loss: 0.7111 - val_loss: 0.6326\n","23/23 [==============================] - 1s 38ms/step - loss: 0.5209\n","23/23 [==============================] - 5s 39ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 30s 144ms/step - loss: 0.9745 - val_loss: 0.5757\n","Epoch 2/10\n","92/92 [==============================] - 10s 107ms/step - loss: 0.6831 - val_loss: 0.5270\n","Epoch 3/10\n","92/92 [==============================] - 9s 103ms/step - loss: 0.5439 - val_loss: 0.6266\n","Epoch 4/10\n","92/92 [==============================] - 10s 105ms/step - loss: 0.7124 - val_loss: 0.7000\n","23/23 [==============================] - 1s 41ms/step - loss: 0.5270\n","23/23 [==============================] - 5s 41ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"]},{"output_type":"stream","name":"stdout","text":["   Dataset    Model Distant      Loss  Accuracy  Precision    Recall  \\\n","0      sg1  roberta   False  0.580312  0.515538   0.562240  0.636251   \n","1      sg2  roberta   False  0.512574  0.545579   0.522718  0.595028   \n","2      sg1  deberta   False  1.786612  0.513528   0.431187  0.633163   \n","3      sg2  deberta   False  2.062078  0.500138   0.296457  0.600000   \n","4      sg1  electra   False  0.550895  0.475357   0.444049  0.380805   \n","5      sg2  electra   False  0.482666  0.506686   0.651247  0.436464   \n","6      sg1  deberta    True  0.617424  0.492855   0.334588  0.584009   \n","7      sg2  deberta    True  0.620772  0.412249   0.402475  0.313812   \n","8      sg1     bert    True  1.253572  0.452740   0.246955  0.457718   \n","9      sg2     bert    True  0.532223  0.558989   0.539072  0.743646   \n","10     sg1  roberta    True  0.546565  0.468920   0.549649  0.386568   \n","11     sg2  roberta    True  0.498680  0.570941   0.598381  0.611602   \n","\n","          F1  F1 Micro  F1 Weighted  \n","0   0.460742  0.515538     0.377869  \n","1   0.418805  0.545579     0.419941  \n","2   0.467586  0.513528     0.389934  \n","3   0.396837  0.500138     0.335278  \n","4   0.393093  0.475357     0.448213  \n","5   0.394047  0.506686     0.440985  \n","6   0.418621  0.492855     0.396586  \n","7   0.241562  0.412249     0.310578  \n","8   0.316711  0.452740     0.320406  \n","9   0.606827  0.558989     0.502989  \n","10  0.330550  0.468920     0.378031  \n","11  0.549337  0.570941     0.522579  \n"]}]},{"cell_type":"markdown","source":["# DeBERTa w/ Distant"],"metadata":{"id":"jsghQ_C0qqzH"}},{"cell_type":"code","source":["# Sg1 \n","results = measure(run_model_5fold('sg1', df_sg1, 'deberta', freeze_encoder=False, pretrained=True, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k8bZ4S1nq3Cd","executionInfo":{"status":"ok","timestamp":1668897367246,"user_tz":420,"elapsed":587129,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"a00a4871-6717-4fee-aef4-8c8bc47c5a1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 55s 505ms/step - loss: 0.7873 - val_loss: 0.6543\n","Epoch 2/10\n","39/39 [==============================] - 12s 320ms/step - loss: 0.7209 - val_loss: 0.7050\n","Epoch 3/10\n","39/39 [==============================] - 13s 327ms/step - loss: 0.7012 - val_loss: 0.6984\n","10/10 [==============================] - 0s 44ms/step - loss: 0.6543\n","10/10 [==============================] - 7s 45ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 56s 511ms/step - loss: 1.4227 - val_loss: 0.6928\n","Epoch 2/10\n","39/39 [==============================] - 13s 321ms/step - loss: 1.4100 - val_loss: 0.6964\n","Epoch 3/10\n","39/39 [==============================] - 13s 330ms/step - loss: 1.5032 - val_loss: 0.6432\n","Epoch 4/10\n","39/39 [==============================] - 13s 322ms/step - loss: 1.3934 - val_loss: 0.7428\n","Epoch 5/10\n","39/39 [==============================] - 13s 325ms/step - loss: 1.3821 - val_loss: 0.6929\n","10/10 [==============================] - 1s 51ms/step - loss: 0.6432\n","10/10 [==============================] - 7s 52ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 56s 509ms/step - loss: 1.2985 - val_loss: 0.7121\n","Epoch 2/10\n","39/39 [==============================] - 13s 329ms/step - loss: 0.7106 - val_loss: 0.6951\n","Epoch 3/10\n","39/39 [==============================] - 13s 328ms/step - loss: 0.7047 - val_loss: 0.6934\n","Epoch 4/10\n","39/39 [==============================] - 12s 320ms/step - loss: 0.7040 - val_loss: 0.7028\n","Epoch 5/10\n","39/39 [==============================] - 13s 324ms/step - loss: 0.6897 - val_loss: 0.7090\n","10/10 [==============================] - 0s 44ms/step - loss: 0.6934\n","10/10 [==============================] - 7s 49ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 56s 509ms/step - loss: 1.6339 - val_loss: 0.6588\n","Epoch 2/10\n","39/39 [==============================] - 12s 319ms/step - loss: 0.6686 - val_loss: 0.6659\n","Epoch 3/10\n","39/39 [==============================] - 13s 330ms/step - loss: 0.6466 - val_loss: 0.6027\n","Epoch 4/10\n","39/39 [==============================] - 13s 328ms/step - loss: 0.5569 - val_loss: 0.5793\n","Epoch 5/10\n","39/39 [==============================] - 12s 319ms/step - loss: 0.6418 - val_loss: 0.6071\n","Epoch 6/10\n","39/39 [==============================] - 13s 324ms/step - loss: 0.6891 - val_loss: 0.8038\n","10/10 [==============================] - 0s 43ms/step - loss: 0.5793\n","10/10 [==============================] - 7s 42ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 56s 504ms/step - loss: 0.7632 - val_loss: 0.5815\n","Epoch 2/10\n","39/39 [==============================] - 13s 324ms/step - loss: 0.5804 - val_loss: 0.5169\n","Epoch 3/10\n","39/39 [==============================] - 12s 316ms/step - loss: 0.6132 - val_loss: 1.0005\n","Epoch 4/10\n","39/39 [==============================] - 12s 319ms/step - loss: 0.7122 - val_loss: 0.7070\n","10/10 [==============================] - 1s 53ms/step - loss: 0.5169\n","10/10 [==============================] - 7s 54ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"]},{"output_type":"stream","name":"stdout","text":["  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538   0.562240  0.636251  0.460742   \n","1     sg2  roberta   False  0.512574  0.545579   0.522718  0.595028  0.418805   \n","2     sg1  deberta   False  1.786612  0.513528   0.431187  0.633163  0.467586   \n","3     sg2  deberta   False  2.062078  0.500138   0.296457  0.600000  0.396837   \n","4     sg1  electra   False  0.550895  0.475357   0.444049  0.380805  0.393093   \n","5     sg2  electra   False  0.482666  0.506686   0.651247  0.436464  0.394047   \n","6     sg1  deberta    True  0.617424  0.492855   0.334588  0.584009  0.418621   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n","1  0.545579     0.419941  \n","2  0.513528     0.389934  \n","3  0.500138     0.335278  \n","4  0.475357     0.448213  \n","5  0.506686     0.440985  \n","6  0.492855     0.396586  \n"]}]},{"cell_type":"code","source":["# Sg2\n","results = measure(run_model_5fold('sg2', df_sg2, 'deberta', freeze_encoder=False, pretrained=True, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmMHXY1dq2tJ","executionInfo":{"status":"ok","timestamp":1668898262406,"user_tz":420,"elapsed":895173,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"8859bead-e816-4646-95d2-126cf7b67e8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 72s 398ms/step - loss: 0.7597 - val_loss: 0.6905\n","Epoch 2/10\n","92/92 [==============================] - 30s 325ms/step - loss: 0.5920 - val_loss: 0.5679\n","Epoch 3/10\n","92/92 [==============================] - 30s 322ms/step - loss: 0.5050 - val_loss: 0.7358\n","Epoch 4/10\n","92/92 [==============================] - 30s 323ms/step - loss: 0.5439 - val_loss: 0.5810\n","23/23 [==============================] - 1s 46ms/step - loss: 0.5679\n","23/23 [==============================] - 7s 48ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 74s 402ms/step - loss: 1.7427 - val_loss: 0.6280\n","Epoch 2/10\n","92/92 [==============================] - 30s 322ms/step - loss: 0.8309 - val_loss: 0.6976\n","Epoch 3/10\n","92/92 [==============================] - 30s 323ms/step - loss: 0.6906 - val_loss: 0.8114\n","23/23 [==============================] - 1s 45ms/step - loss: 0.6280\n","23/23 [==============================] - 7s 45ms/step\n","### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 74s 402ms/step - loss: 0.7767 - val_loss: 0.6939\n","Epoch 2/10\n","92/92 [==============================] - 30s 325ms/step - loss: 0.6478 - val_loss: 0.5723\n","Epoch 3/10\n","92/92 [==============================] - 30s 323ms/step - loss: 0.8372 - val_loss: 0.5842\n","Epoch 4/10\n","92/92 [==============================] - 30s 325ms/step - loss: 0.5802 - val_loss: 0.5375\n","Epoch 5/10\n","92/92 [==============================] - 29s 320ms/step - loss: 0.5541 - val_loss: 0.6826\n","Epoch 6/10\n","92/92 [==============================] - 30s 322ms/step - loss: 1.0195 - val_loss: 0.6836\n","23/23 [==============================] - 1s 46ms/step - loss: 0.5375\n","23/23 [==============================] - 7s 45ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 74s 402ms/step - loss: 1.5932 - val_loss: 0.7086\n","Epoch 2/10\n","92/92 [==============================] - 30s 321ms/step - loss: 0.7898 - val_loss: 1.1221\n","Epoch 3/10\n","92/92 [==============================] - 30s 323ms/step - loss: 0.6676 - val_loss: 0.7890\n","23/23 [==============================] - 1s 46ms/step - loss: 0.7086\n","23/23 [==============================] - 7s 47ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","All model checkpoint layers were used when initializing TFDebertaForSequenceClassification.\n","\n","Some layers of TFDebertaForSequenceClassification were not initialized from the model checkpoint at kamalkraj/deberta-base and are newly initialized: ['classifier', 'cls_dropout', 'pooler']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 70s 359ms/step - loss: 0.8211 - val_loss: 0.6885\n","Epoch 2/10\n","92/92 [==============================] - 27s 290ms/step - loss: 0.7076 - val_loss: 0.6619\n","Epoch 3/10\n","92/92 [==============================] - 26s 285ms/step - loss: 0.5853 - val_loss: 0.7247\n","Epoch 4/10\n","92/92 [==============================] - 26s 278ms/step - loss: 0.6914 - val_loss: 0.6685\n","23/23 [==============================] - 1s 54ms/step - loss: 0.6619\n","23/23 [==============================] - 8s 55ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"]},{"output_type":"stream","name":"stdout","text":["  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538   0.562240  0.636251  0.460742   \n","1     sg2  roberta   False  0.512574  0.545579   0.522718  0.595028  0.418805   \n","2     sg1  deberta   False  1.786612  0.513528   0.431187  0.633163  0.467586   \n","3     sg2  deberta   False  2.062078  0.500138   0.296457  0.600000  0.396837   \n","4     sg1  electra   False  0.550895  0.475357   0.444049  0.380805  0.393093   \n","5     sg2  electra   False  0.482666  0.506686   0.651247  0.436464  0.394047   \n","6     sg1  deberta    True  0.617424  0.492855   0.334588  0.584009  0.418621   \n","7     sg2  deberta    True  0.620772  0.412249   0.402475  0.313812  0.241562   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n","1  0.545579     0.419941  \n","2  0.513528     0.389934  \n","3  0.500138     0.335278  \n","4  0.475357     0.448213  \n","5  0.506686     0.440985  \n","6  0.492855     0.396586  \n","7  0.412249     0.310578  \n"]}]},{"cell_type":"markdown","source":["# ELECTRA"],"metadata":{"id":"IMrySgjDq41F"}},{"cell_type":"code","source":["# Sg1 \n","results = measure(run_model_5fold('sg1', df_sg1, 'electra', freeze_encoder=False, pretrained=False, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7c015d41835243f4b38f7e6a4c4cd0cf","7465abfb215b4a7ba5de3f0a0cd9f833","ad386fbf477a458db925a67ba13f39b8","425c72a0af0346a8a38e5e67aa8516df","602b66553ae04aa296770d47ad633020","047e6af9e080451f84d415293b2d2149","2a85da80e2154a00b91fb7980214c78e","e5eee8cce7854b26976f1f2ae95133ac","f34183e9c6394dc59a8d75544157405d","be370a7859ce4f0bafeb02f7fbcad03f","d2a4dc2860f2404a8303ff569e6dfd81","edcb1d91c9b84fd7ba6952572a4e3267","0f5bd1e7e03440efbee4c92f6a991e3d","e7d1cf6b3bb94230aa7205e0306cb49c","88d4099af65b4ee0bc0089597fcf371b","177f17ca8f1d46eebf47fbb25caf0ac0","d5e9ab0093784138a8b5d6f090df1afe","a5fbd0c2db904bdeb35b281f8ed6f8a6","44bab96895264916af5fc80cc6887ccf","0c51342dfa13468390ef9fb337faefe4","2240b95d3e1f4ef8ae036df55d9f6ba5","c3a1e6be4ae9452db4ccbe901ce10df5","a49d1c33803a467d812ac0cb5998e62f","6ea8fef3962c4c70952e14970597dbbf","b25d2f4791374ad390ed0baa2b8a85e1","e7486fcd7ca048fba45106578653e369","eaf6ae4b655143b7b6dfec4d6a16f860","0fb5035de24b4a13b4091a919fc275c0","11d64a38a2f44976a9b9fba42d756fa5","2574291c9a2543139e49eecb81fc0a76","d80da758aae641c6ab35e9ba12343d65","653cd5ee87c342f4bb4fc7205ca16e38","96ede6392db545339d4381e7ff3ff363","5ca2fdaf8b6d4b6d94fca3f40b3f2740","3c88786990154517992f46cc18488c0b","3e7e2d3e3e2e4880af79ef842d044c2c","a7107029869945faaab1c3e5de647270","31472b5fa23b4cf5a5b55fe3df21f374","e143763f05a64f66852e6f45d1479107","718d12c25f6e46eaa0fb23cae314fd92","2b7b9441af1d45bb80a7e010a88e4f47","db9be2f51c9b426d9b7854a4de56c5d3","83034783aba94ac89dc8bbf9c43dde4d","9336b434a21c4c5290dcdb250096b47c"]},"id":"ZxOJoqXlq_6g","executionInfo":{"status":"ok","timestamp":1668896474049,"user_tz":420,"elapsed":235944,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"e5c338de-c3a2-45bd-e014-bbffa14015ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c015d41835243f4b38f7e6a4c4cd0cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edcb1d91c9b84fd7ba6952572a4e3267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a49d1c33803a467d812ac0cb5998e62f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/54.5M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ca2fdaf8b6d4b6d94fca3f40b3f2740"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 25s 142ms/step - loss: 1.0961 - val_loss: 0.7138\n","Epoch 2/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.7017 - val_loss: 0.6852\n","Epoch 3/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6779 - val_loss: 0.6474\n","Epoch 4/10\n","39/39 [==============================] - 2s 60ms/step - loss: 0.6264 - val_loss: 0.6301\n","Epoch 5/10\n","39/39 [==============================] - 2s 60ms/step - loss: 0.6104 - val_loss: 0.5357\n","Epoch 6/10\n","39/39 [==============================] - 2s 60ms/step - loss: 0.4556 - val_loss: 0.4713\n","Epoch 7/10\n","39/39 [==============================] - 2s 58ms/step - loss: 0.3822 - val_loss: 0.6325\n","Epoch 8/10\n","39/39 [==============================] - 2s 60ms/step - loss: 0.2550 - val_loss: 0.8547\n","10/10 [==============================] - 0s 26ms/step - loss: 0.4713\n","10/10 [==============================] - 3s 26ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 23s 160ms/step - loss: 1.2015 - val_loss: 0.7031\n","Epoch 2/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6971 - val_loss: 0.6890\n","Epoch 3/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6682 - val_loss: 0.6182\n","Epoch 4/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6088 - val_loss: 0.5948\n","Epoch 5/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.5156 - val_loss: 0.5188\n","Epoch 6/10\n","39/39 [==============================] - 2s 58ms/step - loss: 0.3870 - val_loss: 0.7727\n","Epoch 7/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.2498 - val_loss: 1.0063\n","10/10 [==============================] - 0s 26ms/step - loss: 0.5188\n","10/10 [==============================] - 3s 27ms/step\n","### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 23s 161ms/step - loss: 0.8942 - val_loss: 0.6923\n","Epoch 2/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6892 - val_loss: 0.6843\n","Epoch 3/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6553 - val_loss: 0.6302\n","Epoch 4/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.5722 - val_loss: 0.6028\n","Epoch 5/10\n","39/39 [==============================] - 2s 58ms/step - loss: 0.4381 - val_loss: 0.6169\n","Epoch 6/10\n","39/39 [==============================] - 2s 60ms/step - loss: 0.4351 - val_loss: 0.6160\n","10/10 [==============================] - 0s 26ms/step - loss: 0.6028\n","10/10 [==============================] - 3s 25ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 23s 160ms/step - loss: 1.0848 - val_loss: 0.7142\n","Epoch 2/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.7022 - val_loss: 0.6896\n","Epoch 3/10\n","39/39 [==============================] - 2s 60ms/step - loss: 0.6880 - val_loss: 0.6887\n","Epoch 4/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6244 - val_loss: 0.5915\n","Epoch 5/10\n","39/39 [==============================] - 2s 59ms/step - loss: 0.5295 - val_loss: 0.7560\n","Epoch 6/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.5126 - val_loss: 0.5937\n","10/10 [==============================] - 0s 25ms/step - loss: 0.5915\n","10/10 [==============================] - 3s 26ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 23s 162ms/step - loss: 1.9510 - val_loss: 0.7534\n","Epoch 2/10\n","39/39 [==============================] - 2s 62ms/step - loss: 0.7208 - val_loss: 0.6937\n","Epoch 3/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6942 - val_loss: 0.6861\n","Epoch 4/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6853 - val_loss: 0.6661\n","Epoch 5/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.6066 - val_loss: 0.6025\n","Epoch 6/10\n","39/39 [==============================] - 2s 59ms/step - loss: 0.5294 - val_loss: 1.0345\n","Epoch 7/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.4539 - val_loss: 0.5701\n","Epoch 8/10\n","39/39 [==============================] - 2s 59ms/step - loss: 0.3396 - val_loss: 1.8429\n","Epoch 9/10\n","39/39 [==============================] - 2s 61ms/step - loss: 0.2839 - val_loss: 1.7202\n","10/10 [==============================] - 0s 27ms/step - loss: 0.5701\n","10/10 [==============================] - 3s 27ms/step\n","  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538   0.562240  0.636251  0.460742   \n","1     sg2  roberta   False  0.512574  0.545579   0.522718  0.595028  0.418805   \n","2     sg1  deberta   False  1.786612  0.513528   0.431187  0.633163  0.467586   \n","3     sg2  deberta   False  2.062078  0.500138   0.296457  0.600000  0.396837   \n","4     sg1  electra   False  0.550895  0.475357   0.444049  0.380805  0.393093   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n","1  0.545579     0.419941  \n","2  0.513528     0.389934  \n","3  0.500138     0.335278  \n","4  0.475357     0.448213  \n"]}]},{"cell_type":"code","source":["# Sg2 \n","results = measure(run_model_5fold('sg2', df_sg2, 'electra', freeze_encoder=False, pretrained=False, plot=False), results)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MuRWhHndrALg","executionInfo":{"status":"ok","timestamp":1668896780131,"user_tz":420,"elapsed":306094,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"b4df71dd-ec89-4301-e478-a19016cb7aab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["### Start fold 1\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 25s 93ms/step - loss: 0.8252 - val_loss: 0.6461\n","Epoch 2/10\n","92/92 [==============================] - 5s 59ms/step - loss: 0.5869 - val_loss: 0.5166\n","Epoch 3/10\n","92/92 [==============================] - 5s 58ms/step - loss: 0.5222 - val_loss: 0.5501\n","Epoch 4/10\n","92/92 [==============================] - 5s 58ms/step - loss: 0.4234 - val_loss: 0.5579\n","23/23 [==============================] - 1s 27ms/step - loss: 0.5166\n","23/23 [==============================] - 4s 26ms/step\n","### Start fold 2\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 26s 102ms/step - loss: 0.8490 - val_loss: 0.6674\n","Epoch 2/10\n","92/92 [==============================] - 6s 60ms/step - loss: 0.5922 - val_loss: 0.4950\n","Epoch 3/10\n","92/92 [==============================] - 5s 59ms/step - loss: 0.4859 - val_loss: 0.4766\n","Epoch 4/10\n","92/92 [==============================] - 5s 58ms/step - loss: 0.4076 - val_loss: 0.5213\n","Epoch 5/10\n","92/92 [==============================] - 5s 59ms/step - loss: 0.3430 - val_loss: 0.6851\n","23/23 [==============================] - 1s 26ms/step - loss: 0.4766\n","23/23 [==============================] - 4s 26ms/step\n","### Start fold 3\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 26s 101ms/step - loss: 0.8817 - val_loss: 0.6276\n","Epoch 2/10\n","92/92 [==============================] - 5s 60ms/step - loss: 0.5570 - val_loss: 0.4823\n","Epoch 3/10\n","92/92 [==============================] - 5s 59ms/step - loss: 0.4527 - val_loss: 0.4667\n","Epoch 4/10\n","92/92 [==============================] - 5s 59ms/step - loss: 0.3757 - val_loss: 0.4552\n","Epoch 5/10\n","92/92 [==============================] - 5s 59ms/step - loss: 0.3480 - val_loss: 0.9295\n","Epoch 6/10\n","92/92 [==============================] - 5s 59ms/step - loss: 0.3137 - val_loss: 0.5345\n","23/23 [==============================] - 1s 26ms/step - loss: 0.4552\n","23/23 [==============================] - 4s 26ms/step\n","### Start fold 4\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 26s 102ms/step - loss: 0.8090 - val_loss: 0.6038\n","Epoch 2/10\n","92/92 [==============================] - 5s 60ms/step - loss: 0.5422 - val_loss: 0.5121\n","Epoch 3/10\n","92/92 [==============================] - 5s 59ms/step - loss: 0.4488 - val_loss: 0.5146\n","Epoch 4/10\n","92/92 [==============================] - 5s 59ms/step - loss: 0.4092 - val_loss: 0.5190\n","23/23 [==============================] - 1s 27ms/step - loss: 0.5121\n","23/23 [==============================] - 4s 26ms/step\n","### Start fold 5\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n","- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","92/92 [==============================] - 25s 99ms/step - loss: 0.8364 - val_loss: 0.6593\n","Epoch 2/10\n","92/92 [==============================] - 5s 57ms/step - loss: 0.6018 - val_loss: 0.5877\n","Epoch 3/10\n","92/92 [==============================] - 5s 57ms/step - loss: 0.4835 - val_loss: 0.4560\n","Epoch 4/10\n","92/92 [==============================] - 5s 56ms/step - loss: 0.5616 - val_loss: 0.5526\n","Epoch 5/10\n","92/92 [==============================] - 5s 58ms/step - loss: 0.4302 - val_loss: 0.4528\n","Epoch 6/10\n","92/92 [==============================] - 5s 56ms/step - loss: 0.3089 - val_loss: 0.5342\n","Epoch 7/10\n","92/92 [==============================] - 5s 57ms/step - loss: 0.2496 - val_loss: 0.8510\n","23/23 [==============================] - 1s 26ms/step - loss: 0.4528\n","23/23 [==============================] - 4s 27ms/step\n","  Dataset    Model Distant      Loss  Accuracy  Precision    Recall        F1  \\\n","0     sg1  roberta   False  0.580312  0.515538   0.562240  0.636251  0.460742   \n","1     sg2  roberta   False  0.512574  0.545579   0.522718  0.595028  0.418805   \n","2     sg1  deberta   False  1.786612  0.513528   0.431187  0.633163  0.467586   \n","3     sg2  deberta   False  2.062078  0.500138   0.296457  0.600000  0.396837   \n","4     sg1  electra   False  0.550895  0.475357   0.444049  0.380805  0.393093   \n","5     sg2  electra   False  0.482666  0.506686   0.651247  0.436464  0.394047   \n","\n","   F1 Micro  F1 Weighted  \n","0  0.515538     0.377869  \n","1  0.545579     0.419941  \n","2  0.513528     0.389934  \n","3  0.500138     0.335278  \n","4  0.475357     0.448213  \n","5  0.506686     0.440985  \n"]}]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"c86NAYcjjm8n"}},{"cell_type":"code","source":["df_sg1.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"8YsWMRMBgNYR","executionInfo":{"status":"ok","timestamp":1668976159385,"user_tz":420,"elapsed":93,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"abc6282d-ab4d-401e-de05-7f063d466cf7"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            sentence  \\\n","0  The Republican president assumed he was helpin...   \n","1  Though the indictment of a woman for her own p...   \n","3  The tragedy of America’s 18 years in Afghanist...   \n","4  The justices threw out a challenge from gun ri...   \n","5  A review of his posts in online message boards...   \n","\n","                                           news_link     outlet  \\\n","0  http://www.msnbc.com/rachel-maddow-show/auto-i...      msnbc   \n","1  https://eu.usatoday.com/story/news/nation/2019...  usa-today   \n","3  http://feedproxy.google.com/~r/breitbart/~3/ER...  breitbart   \n","4  https://www.huffpost.com/entry/supreme-court-g...      msnbc   \n","5  https://eu.usatoday.com/story/news/nation/2020...  usa-today   \n","\n","                                   topic    type  Label_bias  \\\n","0                            environment    left           1   \n","1                               abortion  center           0   \n","3  international-politics-and-world-news   right           1   \n","4                            gun-control    left           0   \n","5                      white-nationalism  center           1   \n","\n","                           label_opinion             biased_words  \n","0             Expresses writer’s opinion                       []  \n","1  Somewhat factual but also opinionated                       []  \n","3  Somewhat factual but also opinionated  ['tragedy', 'stubborn']  \n","4                       Entirely factual                       []  \n","5                       Entirely factual                ['plant']  "],"text/html":["\n","  <div id=\"df-4d1c8d71-30f8-4f1b-9480-59f0a56a65a4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>news_link</th>\n","      <th>outlet</th>\n","      <th>topic</th>\n","      <th>type</th>\n","      <th>Label_bias</th>\n","      <th>label_opinion</th>\n","      <th>biased_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Republican president assumed he was helpin...</td>\n","      <td>http://www.msnbc.com/rachel-maddow-show/auto-i...</td>\n","      <td>msnbc</td>\n","      <td>environment</td>\n","      <td>left</td>\n","      <td>1</td>\n","      <td>Expresses writer’s opinion</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Though the indictment of a woman for her own p...</td>\n","      <td>https://eu.usatoday.com/story/news/nation/2019...</td>\n","      <td>usa-today</td>\n","      <td>abortion</td>\n","      <td>center</td>\n","      <td>0</td>\n","      <td>Somewhat factual but also opinionated</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The tragedy of America’s 18 years in Afghanist...</td>\n","      <td>http://feedproxy.google.com/~r/breitbart/~3/ER...</td>\n","      <td>breitbart</td>\n","      <td>international-politics-and-world-news</td>\n","      <td>right</td>\n","      <td>1</td>\n","      <td>Somewhat factual but also opinionated</td>\n","      <td>['tragedy', 'stubborn']</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The justices threw out a challenge from gun ri...</td>\n","      <td>https://www.huffpost.com/entry/supreme-court-g...</td>\n","      <td>msnbc</td>\n","      <td>gun-control</td>\n","      <td>left</td>\n","      <td>0</td>\n","      <td>Entirely factual</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>A review of his posts in online message boards...</td>\n","      <td>https://eu.usatoday.com/story/news/nation/2020...</td>\n","      <td>usa-today</td>\n","      <td>white-nationalism</td>\n","      <td>center</td>\n","      <td>1</td>\n","      <td>Entirely factual</td>\n","      <td>['plant']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d1c8d71-30f8-4f1b-9480-59f0a56a65a4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4d1c8d71-30f8-4f1b-9480-59f0a56a65a4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4d1c8d71-30f8-4f1b-9480-59f0a56a65a4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["df_sg2.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"WDBmhX0HgRmB","executionInfo":{"status":"ok","timestamp":1668972361802,"user_tz":420,"elapsed":6,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"b960bc1e-035d-44d5-fe89-9cd975d2eab2"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            sentence  \\\n","0  \"Orange Is the New Black\" star Yael Stone is r...   \n","1  \"We have one beautiful law,\" Trump recently sa...   \n","2  ...immigrants as criminals and eugenics, all o...   \n","3  ...we sounded the alarm in the early months of...   \n","4  [Black Lives Matter] is essentially a non-fals...   \n","\n","                                           news_link     outlet  \\\n","0  https://www.foxnews.com/entertainment/australi...   Fox News   \n","1  https://www.alternet.org/2020/06/law-and-order...   Alternet   \n","2  https://www.nbcnews.com/news/latino/after-step...      MSNBC   \n","3  https://www.alternet.org/2019/07/fox-news-has-...   Alternet   \n","4  http://feedproxy.google.com/~r/breitbart/~3/-v...  Breitbart   \n","\n","               topic   type  Label_bias  \\\n","0        environment  right           0   \n","1        gun control   left           1   \n","2  white-nationalism   left           1   \n","3  white-nationalism   left           1   \n","4  marriage-equality    NaN           1   \n","\n","                           label_opinion                        biased_words  \n","0                       Entirely factual                                  []  \n","1  Somewhat factual but also opinionated   ['bizarre', 'characteristically']  \n","2             Expresses writer’s opinion  ['criminals', 'fringe', 'extreme']  \n","3  Somewhat factual but also opinionated                                  []  \n","4             Expresses writer’s opinion                            ['cult']  "],"text/html":["\n","  <div id=\"df-bf788d45-1ca7-45da-9cd3-7d658cb4e8f0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>news_link</th>\n","      <th>outlet</th>\n","      <th>topic</th>\n","      <th>type</th>\n","      <th>Label_bias</th>\n","      <th>label_opinion</th>\n","      <th>biased_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n","      <td>https://www.foxnews.com/entertainment/australi...</td>\n","      <td>Fox News</td>\n","      <td>environment</td>\n","      <td>right</td>\n","      <td>0</td>\n","      <td>Entirely factual</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n","      <td>https://www.alternet.org/2020/06/law-and-order...</td>\n","      <td>Alternet</td>\n","      <td>gun control</td>\n","      <td>left</td>\n","      <td>1</td>\n","      <td>Somewhat factual but also opinionated</td>\n","      <td>['bizarre', 'characteristically']</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>...immigrants as criminals and eugenics, all o...</td>\n","      <td>https://www.nbcnews.com/news/latino/after-step...</td>\n","      <td>MSNBC</td>\n","      <td>white-nationalism</td>\n","      <td>left</td>\n","      <td>1</td>\n","      <td>Expresses writer’s opinion</td>\n","      <td>['criminals', 'fringe', 'extreme']</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>...we sounded the alarm in the early months of...</td>\n","      <td>https://www.alternet.org/2019/07/fox-news-has-...</td>\n","      <td>Alternet</td>\n","      <td>white-nationalism</td>\n","      <td>left</td>\n","      <td>1</td>\n","      <td>Somewhat factual but also opinionated</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[Black Lives Matter] is essentially a non-fals...</td>\n","      <td>http://feedproxy.google.com/~r/breitbart/~3/-v...</td>\n","      <td>Breitbart</td>\n","      <td>marriage-equality</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>Expresses writer’s opinion</td>\n","      <td>['cult']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf788d45-1ca7-45da-9cd3-7d658cb4e8f0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bf788d45-1ca7-45da-9cd3-7d658cb4e8f0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bf788d45-1ca7-45da-9cd3-7d658cb4e8f0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# sg1 sentences\n","fact_bias = df_sg1.loc[(df_sg1['label_opinion']=='Entirely factual') & (df_sg1['Label_bias']==1)].sample().iloc[0]\n","fact_nonbias = df_sg1.loc[(df_sg1['label_opinion']=='Entirely factual') & (df_sg1['Label_bias']==0)].sample().iloc[0]\n","opin_bias = df_sg1.loc[(df_sg1['label_opinion']=='Expresses writer’s opinion') & (df_sg1['Label_bias']==1)].sample().iloc[0]\n","opin_nonbias = df_sg1.loc[(df_sg1['label_opinion']=='Expresses writer’s opinion') & (df_sg1['Label_bias']==0)].sample().iloc[0]\n","both_bias = df_sg1.loc[(df_sg1['label_opinion']=='Somewhat factual but also opinionated') & (df_sg1['Label_bias']==1)].sample().iloc[0]\n","both_nonbias = df_sg1.loc[(df_sg1['label_opinion']=='Somewhat factual but also opinionated') & (df_sg1['Label_bias']==0)].sample().iloc[0]\n","\n","sentences = [fact_bias, fact_nonbias, opin_bias, opin_nonbias, both_bias, both_nonbias]"],"metadata":{"id":"0qgaqYLEhBv3","executionInfo":{"status":"ok","timestamp":1668976955616,"user_tz":420,"elapsed":83,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# BERT SG1\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/bert_False_sg1_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"LIHPXchPkdiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOaobo26ipCr","executionInfo":{"status":"ok","timestamp":1668977203993,"user_tz":420,"elapsed":1008,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"46545471-9ab7-4756-d685-53c6019fd1f9"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  While sexual ambiguity does occasionally occur...         0      1  \n","1  George Washington University (GW)’s Parliament...         0      0  \n","2  Perhaps that is why both Warren and Sanders ha...         1      1  \n","3  Despite a pandemic, one-sided presidential nom...         1      0  \n","4  Before President Donald Trump threatened to vi...         0      1  \n","5  At the dawn of the year 2010, few Americans co...         0      0  \n"]}]},{"cell_type":"code","source":["# BERT Distant SG1\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/bert_True_sg1_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","gc.collect()"],"metadata":{"id":"zervSd4Wh2cN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69B2U314uPlP","executionInfo":{"status":"ok","timestamp":1668977439345,"user_tz":420,"elapsed":1051,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"34477c7e-dc67-47e5-f190-9ba2858c0fa9"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  While sexual ambiguity does occasionally occur...         1      1  \n","1  George Washington University (GW)’s Parliament...         0      0  \n","2  Perhaps that is why both Warren and Sanders ha...         1      1  \n","3  Despite a pandemic, one-sided presidential nom...         0      0  \n","4  Before President Donald Trump threatened to vi...         1      1  \n","5  At the dawn of the year 2010, few Americans co...         0      0  \n"]}]},{"cell_type":"code","source":["# RoBERTa Distant SG1\n","model = TFBertForSequenceClassification.from_pretrained('roberta-base')\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/roberta_True_sg1_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"],"metadata":{"id":"ZOTrlkyclq4Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5sbj0mDuvEZW","executionInfo":{"status":"ok","timestamp":1668977687219,"user_tz":420,"elapsed":1086,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"1dc3e3c1-67ca-4e23-a514-be7b8bba3d2e"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  While sexual ambiguity does occasionally occur...         0      1  \n","1  George Washington University (GW)’s Parliament...         1      0  \n","2  Perhaps that is why both Warren and Sanders ha...         0      1  \n","3  Despite a pandemic, one-sided presidential nom...         0      0  \n","4  Before President Donald Trump threatened to vi...         1      1  \n","5  At the dawn of the year 2010, few Americans co...         0      0  \n"]}]},{"cell_type":"code","source":["# DeBERTa SG1\n","model = TFBertForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/deberta_False_sg1_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = DebertaTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n","\n","gc.collect()"],"metadata":{"id":"sLOb97v2lYBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ab73uch0vjE3","executionInfo":{"status":"ok","timestamp":1668978153630,"user_tz":420,"elapsed":1892,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"4e763735-5fef-4874-b943-f50db86ebcad"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  While sexual ambiguity does occasionally occur...         0      1  \n","1  George Washington University (GW)’s Parliament...         1      0  \n","2  Perhaps that is why both Warren and Sanders ha...         0      1  \n","3  Despite a pandemic, one-sided presidential nom...         0      0  \n","4  Before President Donald Trump threatened to vi...         0      1  \n","5  At the dawn of the year 2010, few Americans co...         0      0  \n"]}]},{"cell_type":"code","source":["# DeBERTa Distant SG1\n","model = TFBertForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/deberta_True_sg1_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = DebertaTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n","\n","gc.collect()"],"metadata":{"id":"U7Qr5UOilMnm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64dx1QHOvlCW","executionInfo":{"status":"ok","timestamp":1668978765905,"user_tz":420,"elapsed":1775,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"8b24daf0-dea3-4383-d724-5903a4df06c5"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  While sexual ambiguity does occasionally occur...         0      1  \n","1  George Washington University (GW)’s Parliament...         0      0  \n","2  Perhaps that is why both Warren and Sanders ha...         0      1  \n","3  Despite a pandemic, one-sided presidential nom...         0      0  \n","4  Before President Donald Trump threatened to vi...         0      1  \n","5  At the dawn of the year 2010, few Americans co...         0      0  \n"]}]},{"cell_type":"code","source":["# ELECTRA SG1\n","model = TFBertForSequenceClassification.from_pretrained('google/electra-small-discriminator')\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/electra_False_sg1_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n","\n","gc.collect()"],"metadata":{"id":"Tj6JAXsSlz2X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4sdbTlEvv_GC","executionInfo":{"status":"ok","timestamp":1668978995055,"user_tz":420,"elapsed":1083,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"2023644b-d0ac-499e-a808-010a67fcf4fd"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  While sexual ambiguity does occasionally occur...         1      1  \n","1  George Washington University (GW)’s Parliament...         0      0  \n","2  Perhaps that is why both Warren and Sanders ha...         1      1  \n","3  Despite a pandemic, one-sided presidential nom...         1      0  \n","4  Before President Donald Trump threatened to vi...         1      1  \n","5  At the dawn of the year 2010, few Americans co...         0      0  \n"]}]},{"cell_type":"code","source":["# sg2 sentences\n","fact_bias = df_sg2.loc[(df_sg2['label_opinion']=='Entirely factual') & (df_sg2['Label_bias']==1)].sample().iloc[0]\n","fact_nonbias = df_sg2.loc[(df_sg2['label_opinion']=='Entirely factual') & (df_sg2['Label_bias']==0)].sample().iloc[0]\n","opin_bias = df_sg2.loc[(df_sg2['label_opinion']=='Expresses writer’s opinion') & (df_sg2['Label_bias']==1)].sample().iloc[0]\n","opin_nonbias = df_sg2.loc[(df_sg2['label_opinion']=='Expresses writer’s opinion') & (df_sg2['Label_bias']==0)].sample().iloc[0]\n","both_bias = df_sg2.loc[(df_sg2['label_opinion']=='Somewhat factual but also opinionated') & (df_sg2['Label_bias']==1)].sample().iloc[0]\n","both_nonbias = df_sg2.loc[(df_sg2['label_opinion']=='Somewhat factual but also opinionated') & (df_sg2['Label_bias']==0)].sample().iloc[0]\n","\n","sentences = [fact_bias, fact_nonbias, opin_bias, opin_nonbias, both_bias, both_nonbias]"],"metadata":{"id":"U2utqWl4oLDq","executionInfo":{"status":"ok","timestamp":1668978995055,"user_tz":420,"elapsed":2,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# BERT Distant SG2\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/bert_True_sg2_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","gc.collect()"],"metadata":{"id":"lEWd83mkivNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LSpWCalhwABi","executionInfo":{"status":"ok","timestamp":1668979365879,"user_tz":420,"elapsed":1088,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"d48059ec-5cbc-4f07-d425-09d7cfc3dd4a"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  A tear slipped down the cheek of Ghanaian-Germ...         1      1  \n","1  Swedish police say they are searching for a bl...         1      0  \n","2  The Doctor Who franchise has tested this notio...         1      1  \n","3  Reducing the upper tax rate can often, counter...         0      0  \n","4  While the mayor has severely curtailed religio...         0      1  \n","5  Although Facebook Chief Operating Officer Sher...         1      0  \n"]}]},{"cell_type":"code","source":["# RoBERTa Distant SG2\n","model = TFBertForSequenceClassification.from_pretrained('roberta-base')\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/roberta_True_sg2_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","gc.collect()"],"metadata":{"id":"erOKnVqdjagN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_pi7z19wAue","executionInfo":{"status":"ok","timestamp":1668979731536,"user_tz":420,"elapsed":1051,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"65d2202b-42aa-4003-fb1e-728f12971034"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  A tear slipped down the cheek of Ghanaian-Germ...         0      1  \n","1  Swedish police say they are searching for a bl...         0      0  \n","2  The Doctor Who franchise has tested this notio...         1      1  \n","3  Reducing the upper tax rate can often, counter...         0      0  \n","4  While the mayor has severely curtailed religio...         0      1  \n","5  Although Facebook Chief Operating Officer Sher...         0      0  \n"]}]},{"cell_type":"code","source":["# DeBERTa SG2\n","model = TFBertForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/deberta_False_sg2_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = DebertaTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n","\n","gc.collect()"],"metadata":{"id":"PyCrVVmelUnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1XaYxFUEwBs-","executionInfo":{"status":"ok","timestamp":1668980853440,"user_tz":420,"elapsed":1754,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"abc0b80e-c674-466a-b107-c744400b2163"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  A tear slipped down the cheek of Ghanaian-Germ...         0      1  \n","1  Swedish police say they are searching for a bl...         0      0  \n","2  The Doctor Who franchise has tested this notio...         0      1  \n","3  Reducing the upper tax rate can often, counter...         0      0  \n","4  While the mayor has severely curtailed religio...         0      1  \n","5  Although Facebook Chief Operating Officer Sher...         0      0  \n"]}]},{"cell_type":"code","source":["# DeBERTa Distant SG2\n","model = TFBertForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/deberta_True_sg2_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = DebertaTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n","\n","gc.collect()"],"metadata":{"id":"uaLjRopRlRnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YnL7XbO-wCdY","executionInfo":{"status":"ok","timestamp":1668981776287,"user_tz":420,"elapsed":1765,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"d5967f63-a890-4c7f-c4db-f85aa90bebd0"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  A tear slipped down the cheek of Ghanaian-Germ...         1      1  \n","1  Swedish police say they are searching for a bl...         1      0  \n","2  The Doctor Who franchise has tested this notio...         1      1  \n","3  Reducing the upper tax rate can often, counter...         1      0  \n","4  While the mayor has severely curtailed religio...         1      1  \n","5  Although Facebook Chief Operating Officer Sher...         1      0  \n"]}]},{"cell_type":"code","source":["# ELECTRA SG2\n","model = TFBertForSequenceClassification.from_pretrained('google/electra-small-discriminator')\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n","model.compile(optimizer=optimizer, loss='binary_crossentropy') \n","model.load_weights(f'/content/drive/MyDrive/Colab Notebooks/weights/electra_False_sg2_main')\n","trained_model_layer = model.get_layer(index=0).get_weights()\n","model.get_layer(index=0).set_weights(trained_model_layer)\n","\n","tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n","\n","gc.collect()"],"metadata":{"id":"PlrAUT-7l6xz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['Opinon Label', 'Sentence', 'Predicted', 'Actual']\n","output = pd.DataFrame(columns=cols)\n","\n","for s in sentences:\n","  inputs = tokenizer(s['sentence'], return_tensors=\"tf\")\n","  logits = model(**inputs).logits\n","  predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","  row = {'Opinon Label': s['label_opinion'], \n","         'Sentence': s['sentence'], \n","         'Predicted': predicted_class_id, \n","         'Actual': s['Label_bias']\n","    }\n","    \n","  output = output.append(row, ignore_index=True)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SmIY9NLKwDWC","executionInfo":{"status":"ok","timestamp":1668982081337,"user_tz":420,"elapsed":1107,"user":{"displayName":"John Stilb","userId":"12941440883804019358"}},"outputId":"a78eca36-e538-40d7-9839-4bc048044dbb"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["                            Opinon Label  \\\n","0                       Entirely factual   \n","1                       Entirely factual   \n","2             Expresses writer’s opinion   \n","3             Expresses writer’s opinion   \n","4  Somewhat factual but also opinionated   \n","5  Somewhat factual but also opinionated   \n","\n","                                            Sentence Predicted Actual  \n","0  A tear slipped down the cheek of Ghanaian-Germ...         0      1  \n","1  Swedish police say they are searching for a bl...         0      0  \n","2  The Doctor Who franchise has tested this notio...         1      1  \n","3  Reducing the upper tax rate can often, counter...         0      0  \n","4  While the mayor has severely curtailed religio...         0      1  \n","5  Although Facebook Chief Operating Officer Sher...         0      0  \n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hmM2MfLVy4io"},"execution_count":null,"outputs":[]}]}